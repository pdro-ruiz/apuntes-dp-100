{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Almacenes de datos\n",
    "\n",
    "Concepto\n",
    "Los almacenes de datos de AzureML son abstracciones para fuentes de datos en la nube.\n",
    "\n",
    "Funciones:\n",
    "- Encapsulan información para la conexión a fuentes de datos.\n",
    "- Permiten acceso directo a través del SDK de AzureML.\n",
    "- Facilitan la carga y descarga de datos.\n",
    "\n",
    "Tipos de Almacenes de Datos Soportados:\n",
    "- Azure Blob Storage\n",
    "- Azure File Storage\n",
    "- Azure Data Lake Store\n",
    "- Azure SQL Database\n",
    "- Azure Databricks File System (DBFS)\n",
    "- Entre los mas usados.. (Para la lista completa, consulte la documentación de AzureML)\n",
    "\n",
    "Almacenes de Datos Predeterminados:\n",
    "- Cada espacio de trabajo tiene dos almacenes predeterminados:\n",
    "  - Contenedor Blob de Azure Storage\n",
    "  - Contenedor de archivos de Azure Storage\n",
    "- Se utilizan para el almacenamiento del sistema de AzureML.\n",
    "\n",
    "Tercer Almacén de Datos:\n",
    "Se agrega automáticamente al usar conjuntos de datos de ejemplo.\n",
    "\n",
    "Uso en Proyectos:\n",
    "- En la mayoría de los proyectos, necesitará usar sus propias fuentes de datos.Las razones pueden ser:\n",
    "  1. Manejar mayores volúmenes de datos.\n",
    "  2. Integrar la solución con datos de aplicaciones existentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear y administrar Almacenes de datos\n",
    "\n",
    "#### Registro:\n",
    "\n",
    "- Interfaz gráfica de AzureML Studio.\n",
    "- SDK de AzureML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registrar un contenedor Blob de Azure Storage llamado \"blob_data\"\n",
    "from azureml.core import Workspace, Datastore\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Registramos el nuevo Datastore\n",
    "blob_ds = Datastore.register_azure_blob_container(\n",
    "    workspace=ws, \n",
    "    datastore_name='blob_data', \n",
    "    container_name='data', \n",
    "    account_name='storageaccountname', \n",
    "    container_name='data_container', \n",
    "    account_name='az_store...', \n",
    "    account_key='storageaccountkey'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Visualización y Gestión:\n",
    "\n",
    "Realizable mediante:\n",
    "- AzureML Studio.\n",
    "- SDK de AzureML.\n",
    "\n",
    "Ejemplo: Listar nombres de almacenes con un bucle for y obtener referencias individuales con el método get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con '/bin/python3' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Listamos los datastores registrados\n",
    "for ds_name in ws.datastores:\n",
    "    print(ds_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Almacenamiento Predeterminado:\n",
    "\n",
    "Incluido en todo espacio de trabajo que se crea.\n",
    "- Recuperable con el método get_default_datastore.\n",
    "- Inicialmente es \"workspace blob store datastore\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos el datastore recién registrado\n",
    "blob_store = Datastore.get(ws, datastore_name='blob_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consideraciones para la Planificación:\n",
    "\n",
    "Azure Blob Storage:\n",
    "- Almacenamiento premium: mejor rendimiento I/O para grandes conjuntos de datos (mayor costo y limitaciones de replicación).\n",
    "\n",
    "Formato de Archivos:\n",
    "- Parquet generalmente ofrece mejor rendimiento que CSV.\n",
    "\n",
    "Acceso y Cambio de Predeterminado:\n",
    "- Acceso por nombre del almacén de datos.\n",
    "- Cambio de predeterminado con el método set_default_datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuramos el datastore por defecto\n",
    "ws.set_default_datastore('blob_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio practico - trabajar con datasets\n",
    "\n",
    "##### Creacion, carga y versiones\n",
    "1. Creación de conjuntos de datos de archivos:\n",
    "   \n",
    "    Los datasets son objetos de datos empaquetados con versiones que se pueden usar en experimentos y pipelines. Son la forma recomendada para trabajar con datos en Azure Machine Learning.\n",
    "\n",
    "    Tipos:\n",
    "\n",
    "    - **Tabular**: Para datos con estructura consistente (ej. DataFrames de Pandas).\n",
    "    - **Archivo**: Para datos no estructurados o procesamiento a nivel de archivo (ej. imágenes).\n",
    "\n",
    "    El código en Python muestra cómo crear un dataset tabular a partir de dos rutas de archivo y registrarlo en el espacio de trabajo.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con '/bin/python3' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Para crear un conjunto de datos tabular mediante el SDK, se usa el  método from_delimited_files de la  clase Dataset.Tabular, como se muestra a continuación:\n",
    "\n",
    "from azureml.core import Dataset\n",
    "\n",
    "blob_ds = ws.get_default_datastore()\n",
    "\n",
    "csv_paths = [(blob_ds, 'data/files/current_data.csv'),          # Archivo current_data.csv en la  carpeta data/files/   \n",
    "            (blob_ds, 'data/files/archive/*.csv')]              # Todos los archivos .csv en la  carpeta data/files/archive/\n",
    "\n",
    "# Usando el método from_delimited_files\n",
    "tab_ds = Dataset.Tabular.from_delimited_files(path=csv_paths)   # from_delimited_files de la  clase Dataset.Tabular\n",
    "\n",
    "# \n",
    "tab_ds = tab_ds.register(workspace=ws, name='csv_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con '/bin/python3' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Para crear un conjunto de datos de archivo mediante el SDK, use el  método from_files de la  clase Dataset.File\n",
    "\n",
    "from azureml.core import Dataset\n",
    "\n",
    "blob_ds = ws.get_default_datastore()\n",
    "\n",
    "file_ds = Dataset.File.from_files(path=(blob_ds, 'data/files/images/*.jpg'))    # from_files de la  clase Dataset.File\n",
    "\n",
    "file_ds = file_ds.register(workspace=ws, name='img_files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Recuperación de conjuntos de datos registrados:\n",
    "\n",
    "    Se puede recuperar el dataset mediante cualquiera de las siguientes técnicas:\n",
    "\n",
    "    - El  atributo `datasets` dictionary de un objeto Workspace.\n",
    "    - Método `get_by_name` o `get_by_id` de la clase Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset\n",
    "\n",
    "ws = Workspace.from_config()                        # Obtenemos el objeto Workspace\n",
    "\n",
    "ds1 = ws.datasets['csv_table']                      # Seleccionamos el conjunto de datos por su nombre \n",
    "\n",
    "ds2 = Dataset.get_by_name(ws, 'img_files')          # Seleccionamos el conjunto de datos por el nombre la su clase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Versionado de conjuntos de datos:\n",
    "\n",
    "    Los conjuntos de datos se pueden versionar, lo que le permite realizar un seguimiento de las versiones históricas y reproducir esos experimentos con datos en el mismo estado.\n",
    "\n",
    "    Puede crear una nueva versión de un conjunto de datos registrándolo con el mismo nombre que un conjunto de datos registrado anteriormente y especificando la  propiedad `create_new_version`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En este ejemplo, los archivos .png de la carpeta images se han agregado al conjunto de datos img_paths utilizado.\n",
    "\n",
    "img_paths = [(blob_ds, 'data/files/images/*.jpg'),\n",
    "             (blob_ds, 'data/files/images/*.png')]\n",
    "\n",
    "file_ds = Dataset.File.from_files(path=img_paths)                                   # Usamos el método from_files para registrar los archivos jpg y png\n",
    "\n",
    "file_ds = file_ds.register(workspace=ws, name='img_files', create_new_version=True) # creamos una nueva versión del conjunto de datos img_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Recuperación de versiones específicas de conjuntos de datos:\n",
    "\n",
    "    Podemos recuperar una versión específica de un conjunto especificando el parámetro version en el método `get_by_name` de la clase Dataset.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ds = Dataset.get_by_name(workspace=ws, name='img_files', version=2)            # Obtenemos la versión 2 del conjunto de datos img_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Manejo y uso\n",
    "\n",
    "5. Trabajar con conjuntos de datos \n",
    "\n",
    "    Podemos leer datos directamente de un conjunto de datos tabular convirtiéndolos en un marco de datos de Pandas o Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = tab_ds.to_pandas_dataframe()\n",
    "print(df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Pasar un conjunto de datos tabular a un script\n",
    "   \n",
    "Cuando necesitemos acceder a un conjunto de datos mediante un script, debemos pasarle el conjunto de datos al script. \n",
    "\n",
    "Hay dos maneras de hacerlo:\n",
    "\n",
    "- **Usar un argumento de script para un conjunto de datos tabulares**\n",
    "\n",
    "    Podemos pasar el conjunto de datos como argumento de script. \n",
    "    \n",
    "    Cuando se adopta este enfoque, el argumento recibido por el script es el identificador único del conjunto de datos en el área de trabajo. En el script, puede obtener el área de trabajo del contexto de ejecución y usarlo para recuperar el conjunto de datos por su identificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ScriptRunConfig\n",
    "\n",
    "env = Environment('my_env')                                                         # Creamos un objeto Environment  \n",
    "packages = CondaDependencies.create(conda_packages=['pip'],                         # Creamos un objeto CondaDependencies con las dependencias necesarias\n",
    "                                    pip_packages=['azureml-defaults',\n",
    "                                                  'azureml-dataprep[pandas]'])\n",
    "env.python.conda_dependencies = packages                                            # Asignamos las dependencias al entorno                             \n",
    "\n",
    "script_config = ScriptRunConfig(source_directory='my_dir',                          # Creamos un objeto ScriptRunConfig   \n",
    "                                script='script.py',\n",
    "                                arguments=['--ds', tab_ds],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script\n",
    "\n",
    "from azureml.core import Run, Dataset\n",
    "\n",
    "parser.add_argument('--ds', type=str, dest='dataset_id')                            # Agregamos un argumento para el conjunto de datos\n",
    "args = parser.parse_args()\n",
    "\n",
    "run = Run.get_context()\n",
    "ws = run.experiment.workspace\n",
    "dataset = Dataset.get_by_id(ws, id=args.dataset_id)                                 # Obtenemos el conjunto de datos por su id\n",
    "data = dataset.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Usar una entrada con nombre para un conjunto de datos tabular**\n",
    "\n",
    "    Como alternativa, podemos pasar un conjunto de datos tabular como una entrada con nombre. En este enfoque, se utiliza el  método `as_named_input` del conjunto de datos para especificar un nombre. \n",
    "\n",
    "    A continuación, en el script, puede recuperar el conjunto de datos por nombre de la colección `input_datasets` del contexto de ejecución sin necesidad de recuperarlo del área de trabajo. Tenga en cuenta que si usa este enfoque, aún debe incluir un argumento de script para el conjunto de datos, aunque en realidad no lo use para recuperar el conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ScriptRunConfig\n",
    "\n",
    "env = Environment('my_env')\n",
    "packages = CondaDependencies.create(conda_packages=['pip'],\n",
    "                                    pip_packages=['azureml-defaults',\n",
    "                                                  'azureml-dataprep[pandas]'])\n",
    "env.python.conda_dependencies = packages\n",
    "\n",
    "script_config = ScriptRunConfig(source_directory='my_dir',\n",
    "                                script='script.py',\n",
    "                                arguments=['--ds', tab_ds.as_named_input('my_dataset')],    # Pasamos el conjunto de datos como argumento\n",
    "                                environment=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script\n",
    "\n",
    "from azureml.core import Run\n",
    "\n",
    "parser.add_argument('--ds', type=str, dest='ds_id')\n",
    "args = parser.parse_args()\n",
    "\n",
    "run = Run.get_context()\n",
    "dataset = run.input_datasets['my_dataset']                                          # Obtenemos el conjunto de datos por su nombre\n",
    "data = dataset.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Trabajar con conjuntos de datos de archivos\n",
    "\n",
    "    Para trabajar con un conjunto de datos de archivos, podemos utilizar el método `to_path()` para devolver una lista de las rutas de archivo encapsuladas por el conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in file_ds.to_path():\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Pasar un conjunto de datos de archivo a un script \n",
    "\n",
    "    Al igual que con los conjuntos de datos tabulares, hay dos formas de pasar un conjunto de datos de archivo a un script. Sin embargo, hay algunas diferencias clave en la forma en que se pasa el conjunto de datos.\n",
    "\n",
    "    - **Usar un argumento de script para un conjunto de datos de archivos**\n",
    "\n",
    "        Puede pasar un conjunto de datos de archivos como argumento de script. A diferencia de lo que ocurre con un conjunto de datos tabular, debe especificar un modo para el argumento del conjunto de datos de archivo, que puede ser `as_download` o `as_mount`. Esto proporciona un punto de acceso que el script puede utilizar para leer los archivos del conjunto de datos. \n",
    "\n",
    "        En la mayoría de los casos, se ha de `as_download`, que copia los archivos en una ubicación temporal en el proceso donde se ejecuta el script. Sin embargo, si está trabajando con una gran cantidad de datos para los que es posible que no haya suficiente espacio de almacenamiento en el proceso, usaremos `as_mount` para transmitir los archivos directamente desde su origen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ScriptRunConfig\n",
    "\n",
    "env = Environment('my_env')\n",
    "packages = CondaDependencies.create(conda_packages=['pip'],\n",
    "                                    pip_packages=['azureml-defaults',\n",
    "                                                  'azureml-dataprep[pandas]'])\n",
    "env.python.conda_dependencies = packages\n",
    "\n",
    "script_config = ScriptRunConfig(source_directory='my_dir',\n",
    "                                script='script.py',\n",
    "                                arguments=['--ds', file_ds.as_download()],                          # Pasamos el conjunto de datos como argumento usando el método as_download\n",
    "                                environment=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Script\n",
    "\n",
    "from azureml.core import Run\n",
    "import glob\n",
    "\n",
    "parser.add_argument('--ds', type=str, dest='ds_ref')\n",
    "args = parser.parse_args()\n",
    "run = Run.get_context()\n",
    "\n",
    "imgs = glob.glob(args.ds_ref + \"/*.jpg\")                                                            # Obtenemos los archivos del conjunto de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Usar una entrada con nombre para un conjunto de datos de archivos**\n",
    "\n",
    "    También puede pasar un conjunto de datos de archivos como una entrada con nombre. En este enfoque, se utiliza el  método as_named_input del conjunto de datos para especificar un nombre antes de especificar el modo de acceso. A continuación, en el script, puede recuperar el conjunto de datos por nombre de la colección input_datasets del contexto de ejecución  y leer los archivos desde allí. \n",
    "\n",
    "    Al igual que con los conjuntos de datos tabulares, si usa una entrada con nombre, debe incluir un argumento de script para el conjunto de datos, aunque en realidad no lo use para recuperar el conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ScriptRunConfig\n",
    "\n",
    "env = Environment('my_env')\n",
    "packages = CondaDependencies.create(conda_packages=['pip'],\n",
    "                                    pip_packages=['azureml-defaults',\n",
    "                                                  'azureml-dataprep[pandas]'])\n",
    "env.python.conda_dependencies = packages\n",
    "\n",
    "script_config = ScriptRunConfig(source_directory='my_dir',\n",
    "                                script='script.py',\n",
    "                                arguments=['--ds', file_ds.as_download()],                       # Pasamos el conjunto de datos como argumento usando el método as_download\n",
    "                                environment=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script\n",
    "\n",
    "from azureml.core import Run\n",
    "import glob\n",
    "\n",
    "parser.add_argument('--ds', type=str, dest='ds_ref')\n",
    "args = parser.parse_args()\n",
    "run = Run.get_context()\n",
    "\n",
    "imgs = glob.glob(args.ds_ref + \"/*.jpg\")                                                           # Obtenemos los archivos del conjunto de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Usar una entrada con nombre para un conjunto de datos de archivos\n",
    "    \n",
    "    En este enfoque, se utiliza el  método `as_named_input` del conjunto de datos para especificar un nombre antes de especificar el modo de acceso. A continuación, en el script, puede recuperar el conjunto de datos por nombre de la colección `input_datasets` del contexto de ejecución y leer los archivos desde allí. \n",
    "\n",
    "    Al igual que con los conjuntos de datos tabulares, si usa una entrada con nombre, debe incluir un argumento de script para el conjunto de datos, aunque en realidad no lo use para recuperar el conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ScriptRunConfig\n",
    "\n",
    "env = Environment('my_env')\n",
    "packages = CondaDependencies.create(conda_packages=['pip'],\n",
    "                                    pip_packages=['azureml-defaults',\n",
    "                                                  'azureml-dataprep[pandas]'])\n",
    "env.python.conda_dependencies = packages\n",
    "\n",
    "script_config = ScriptRunConfig(source_directory='my_dir',\n",
    "                                script='script.py',\n",
    "                                arguments=['--ds', file_ds.as_named_input('my_ds').as_download()],    # Pasamos el conjunto de datos como argumento usando el método as_named_input\n",
    "                                environment=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con '/bin/python3' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Script\n",
    "\n",
    "from azureml.core import Run\n",
    "import glob\n",
    "\n",
    "parser.add_argument('--ds', type=str, dest='ds_ref')\n",
    "args = parser.parse_args()\n",
    "run = Run.get_context()\n",
    "\n",
    "dataset = run.input_datasets['my_ds']                                                        # Obtenemos el conjunto de datos por su nombre \n",
    "imgs= glob.glob(dataset + \"/*.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entornos \n",
    "\n",
    "Los códigos de Python se ejecutan en un entorno virtual, que define la versión del intérprete de Python y los paquetes instalados disponibles. Los entornos suelen gestionarse con Conda o Pip. Para mejorar la portabilidad, se suele crear entornos en contenedores **Docker** alojados en objetivos de cómputo (equipos de desarrollo, máquinas virtuales o clusters en la nube).\n",
    "\n",
    "AzureML gestiona la creación de los entornos y la instalación de paquetes, generalmente mediante contenedores Docker. Puedes especificar los paquetes necesarios y que AzureML cree un entorno para el experimento.\n",
    "\n",
    "Dentro de las soluciones empresariales, es importante conocer los entornos de ejecución del código. Los entornos están encapsulados en la clase `environment`, que permite crearlos y especificar la configuración de ejecución.\n",
    "\n",
    "Opciones de gestión de entornos:\n",
    "\n",
    "- Azure ML: Crea y registra el entorno automáticamente.\n",
    "- Gestión manual: Crea y registra entornos propios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creación de un entorno a partir de un archivo de especificación\n",
    "Podemos utilizar un archivo de especificación Conda o pip para definir los paquetes necesarios en un entorno de Python y utilizarlo para crear un objeto `Environment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de archivo de especificación de entorno que se podria llamar conda.yml\n",
    "\n",
    "name: py_env\n",
    "dependencies:\n",
    "  - numpy\n",
    "  - pandas\n",
    "  - scikit-learn\n",
    "  - pip:\n",
    "    - azureml-defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codigo para trabajar con el archivo conda.yml cy sus variables de entorno.\n",
    "\n",
    "from azureml.core import Environment\n",
    "\n",
    "env = Environment.from_conda_specification(name='training_environment',\n",
    "                                           file_path='./conda.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creación de un entorno a partir de un entorno de Conda existente\n",
    "\n",
    "Si ya tenemos un entorno de Conda existente definido en la estación de trabajo, podemos usarlo para definir nuestro nuevo entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "env = Environment.from_existing_conda_environment(name='training_environment',\n",
    "                                                  conda_environment_name='py_env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creación de un entorno mediante la especificación de paquetes\n",
    "Podemos definir un entorno especificando los paquetes Conda y pip que necesitamos en un objeto CondaDependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "env = Environment('training_environment')\n",
    "deps = CondaDependencies.create(conda_packages=['scikit-learn','pandas','numpy'],\n",
    "                                pip_packages=['azureml-defaults'])\n",
    "env.python.conda_dependencies = deps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configuración de contenedores de entorno\n",
    "\n",
    "Normalmente, los entornos para el script del experimento se crean en contenedores. \n",
    "\n",
    "En el código siguiente se configura un experimento basado en scripts para hospedar el entorno de entorno creado anteriormente en un contenedor (este es el valor predeterminado a menos que use `DockerConfiguration` con un  atributo `use_docker=False`, en cuyo caso el entorno se crea directamente en el destino de proceso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment, ScriptRunConfig\n",
    "from azureml.core.runconfig import DockerConfiguration\n",
    "\n",
    "docker_config = DockerConfiguration(use_docker=True)\n",
    "\n",
    "script_config = ScriptRunConfig(source_directory='my_folder',\n",
    "                                script='my_script.py',\n",
    "                                environment=env,\n",
    "                                docker_runtime_config=docker_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure ML usa una biblioteca de imágenes base para contenedores, eligiendo la base adecuada para el destino de proceso que se especifique (por ejemplo, incluida la compatibilidad de Cuda con el proceso basado en GPU). Si creamos imágenes de contenedor personalizadas y las hemos registrado en un registro de contenedor, podemos invalidar las imágenes base creadas y usar las nuestras propias modificando los atributos de la propiedad docker del entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con '/bin/python3' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "env.docker.base_image='my-base-image'\n",
    "env.docker.base_image_registry='myregistry.azurecr.io/myimage'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como alternativa, podemos crear una imagen a petición en función de la imagen base y la configuración adicional de un `dockerfile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.docker.base_image = None\n",
    "env.docker.base_dockerfile = './Dockerfile'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por defecto, Azure Machine Learning controla las rutas de acceso de Python y las dependencias de paquetes.\n",
    "Si la imagen ya incluye una instalación de Python con las dependencias que necesita, podemos invalidar este comportamiento estableciendo `python.user_managed_dependencies=True` y estableciendo una ruta de acceso explícita de Python para la instalación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.python.user_managed_dependencies=True\n",
    "env.python.interpreter_path = '/opt/miniconda/bin/python'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Registro y reutilización de entornos\n",
    "Con el entorno creado, podemos registrarlo en el área de trabajo y reutilizarlo para futuros experimentos que tengan las mismas dependencias de Python.\n",
    "\n",
    "##### Registro de un entorno\n",
    "usamos el metodo `register` del objeto `Environment` para registrar un entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.register(workspace=ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver los entornos registrados de la siguiente manera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "env_names = Environment.list(workspace=ws)\n",
    "for env_name in env_names:\n",
    "    print('Name:',env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recuperación y uso de un entorno\n",
    "Podemos recuperar un entorno registrado mediante el método `get` de la  clase `Environment` y, a continuación, asignarlo a `ScriptRunConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  En este ejemplo se recupera el entorno registrado training_environment y se asigna a una configuración de ejecución de script:\n",
    "\n",
    "from azureml.core import Environment, ScriptRunConfig\n",
    "\n",
    "training_env = Environment.get(workspace=ws, name='training_environment')\n",
    "\n",
    "script_config = ScriptRunConfig(source_directory='my_folder',\n",
    "                                script='my_script.py',\n",
    "                                environment=training_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos de cómputo en Azure ML\n",
    "\n",
    "Son computadoras físicas o virtuales donde se ejecutan experimentos. Existen distintos tipos para adaptarse a tus necesidades.\n",
    "\n",
    "1. Tipos de objetivos de cómputo\n",
    "    - **Cómputo local**: Ideal para desarrollo y pruebas con poca data.\n",
    "        Se ejecuta en el mismo dispositivo donde inicias el experimento (ej. nuestro pc, en la estacion de trabajo del notebook...).\n",
    "    - **Clúster de cómputo**: Para alta escalabilidad con mucha data o procesamiento paralelo.\n",
    "        Grupos de máquinas virtuales que se expanden o contraen según la demanda/necesidad.\n",
    "    - **Cómputo adjunto**: Aprovecha entornos de cómputo ya existentes en Azure (ej. máquinas virtuales, Databricks).\n",
    "        Útil para cargas de trabajo específicas.\n",
    "\n",
    "2. Objetivos de cómputo para inferencia (solo para Azure ML Studio)\n",
    "    - **Clústeres de inferencia**: Utilizan Azure Kubernetes Service para desplegar modelos entrenados como servicios de inferencia.\n",
    "\n",
    "3. Beneficios de usar objetivos de cómputo\n",
    "    - **Flexibilidad**: Desarrollar y probar en local, luego escalar a producción.\n",
    "    - **Optimización de recursos**: Ejecutar procesos en el objetivo más adecuado (ej. CPU para entrenar, CPU solo para evaluar).\n",
    "    - **Control de costes**: Pagar solo por el uso, iniciar y detener objetivos automáticamente, escalado automático.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Creación de objetivos de computo\n",
    "\n",
    "Las maneras más comunes de crear o asociar un compute target son, usar la página Compute en Azure ML Studio o usar el SDK.\n",
    "\n",
    "##### - Creación de un objetivo de computo administrado con el SDK \n",
    "    \n",
    "Es aquel por Azure ML, como un clúster de proceso de Azure Machine Learning.\n",
    "\n",
    "Para crear un clúster de proceso de Azure Machine Learning, use la  clase `azureml.core.compute.ComputeTarget` y la  clase `AmlCompute`, como se muestra a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "\n",
    "# Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Specify a name for the compute (unique within the workspace)\n",
    "compute_name = 'aml-cluster'\n",
    "\n",
    "# Define compute configuration\n",
    "compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2',      # Creamos un objeto AmlCompute con la configuración necesaria   \n",
    "                                                       min_nodes=0, max_nodes=4,        # Definimos el número mínimo y máximo de nodos\n",
    "                                                       vm_priority='dedicated')         # Definimos la prioridad del clúster\n",
    "\n",
    "# Create the compute\n",
    "aml_cluster = ComputeTarget.create(ws, compute_name, compute_config)                    # Creamos el clúster de cálculo\n",
    "aml_cluster.wait_for_completion(show_output=True)                                       # Esperamos a que se complete la creación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - Adjuntar un objetivo de computo no gestionado con el SDK\n",
    "\n",
    "Un compute target no gestionado es aquel que se define y gestiona fuera del espacio de trabajo de Azure ML; por ejemplo, una máquina virtual Azure o un clúster Azure Databricks.\n",
    "\n",
    "El codigo usado para gestionar los compute targets no administrados es similar al de cómputo administrado, excepto que debe utilizar el método `ComputeTarget.attach()` para adjuntar el cómputo existente en función de sus ajustes de configuración específicos del objetivo.\n",
    "\n",
    "Por ejemplo, este codigo se puede utilizar para adjuntar un clúster Azure Databricks existente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "from azureml.core.compute import ComputeTarget, DatabricksCompute\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "compute_name = 'db_cluster'\n",
    "\n",
    "db_workspace_name = 'db_workspace'\n",
    "db_resource_group = 'db_resource_group'\n",
    "db_access_token = '1234-abc-5678-defg-90...'\n",
    "db_config = DatabricksCompute.attach_configuration(resource_group=db_resource_group,        # Creamos un objeto DatabricksCompute con la configuración necesaria\n",
    "                                                   workspace_name=db_workspace_name,        # Definimos el nombre del clúster de Databricks\n",
    "                                                   access_token=db_access_token)            # Definimos el token de acceso\n",
    "\n",
    "# Create the compute\n",
    "databricks_compute = ComputeTarget.attach(ws, compute_name, db_config)                      # Creamos el clúster de Databricks\n",
    "databricks_compute.wait_for_completion(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - Comprobación de la existencia de objetivos de cálculo\n",
    "\n",
    "Si se quiere comprobar la existencia de un objetivo de cálculo y crear uno nuevo sólo si no hay ninguno con el nombre especificado. \n",
    "Para esto, podemos capturar la excepción `ComputeTargetException`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "compute_name = \"aml-cluster\"\n",
    "\n",
    "# Check if the compute target exists\n",
    "try:\n",
    "    aml_cluster = ComputeTarget(workspace=ws, name=compute_name)\n",
    "    print('Found existing cluster.')\n",
    "except ComputeTargetException:\n",
    "    # If not, create it\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2',\n",
    "                                                           max_nodes=4)\n",
    "    aml_cluster = ComputeTarget.create(ws, compute_name, compute_config)\n",
    "\n",
    "aml_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Uso de los objetivos de computo\n",
    "\n",
    "Una vez se hayan creado o asociado los compute targets en la workstation, puede usarlos para ejecutar cargas de trabajo específicas; como son los experimentos.\n",
    "\n",
    "Para usar un objetivo de computo determinado, puede especificarlo en el parámetro adecuado para una configuración de ejecución de experimentos. \n",
    "\n",
    "Por ejemplo, el siguiente codigo configura un estimador para usar el destino de proceso denominado aml-cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment, ScriptRunConfig\n",
    "\n",
    "compute_name = 'aml-cluster'\n",
    "\n",
    "training_env = Environment.get(workspace=ws, name='training_environment')\n",
    "\n",
    "script_config = ScriptRunConfig(source_directory='my_dir',                      # Creamos un objeto ScriptRunConfig con la configuración necesaria\n",
    "                                script='script.py',                             # Definimos el directorio y el script\n",
    "                                environment=training_env,                       # Definimos el entorno\n",
    "                                compute_target=compute_name)                    # Definimos el clúster de cálculo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando se envía un experimento, la ejecución se pondrá en cola mientras se inicia el objetivo de computación aml-cluster y se crea en él el entorno especificado, y luego la ejecución se procesará en el entorno de computación.\n",
    "\n",
    "En lugar de especificar el nombre del objetivo de computación, puede especificar un objeto `ComputeTarget`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment, ScriptRunConfig\n",
    "from azureml.core.compute import ComputeTarget\n",
    "\n",
    "compute_name = \"aml-cluster\"\n",
    "\n",
    "training_cluster = ComputeTarget(workspace=ws, name=compute_name)\n",
    "\n",
    "training_env = Environment.get(workspace=ws, name='training_environment')\n",
    "\n",
    "script_config = ScriptRunConfig(source_directory='my_dir',                      # Creamos un objeto ScriptRunConfig con la configuración necesaria\n",
    "                                script='script.py',                             # Definimos el directorio y el script\n",
    "                                environment=training_env,                       # Definimos el entorno\n",
    "                                compute_target=training_cluster)                # Definimos el clúster de cálculo con compute_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "En Azure ML, las tareas se ejecutan como experimentos utilizando recursos informáticos y datos.\n",
    "\n",
    "Para los procesos empresariales de ciencia de datos, se recomienda dividir el proceso en tareas individuales y orquestarlas con Pipelines (secuencias de pasos conectados).\n",
    "\n",
    "**Los Pipelines son clave para implementar una solución MLOps efectiva.**\n",
    "\n",
    "- Aclaración sobre el término \"Pipeline\"\n",
    "    \n",
    "    El término \"Pipeline\" se usa mucho en ML con significados diferentes.\n",
    "\n",
    "    - **Scikit-learn**: enlaza preprocesamiento de datos con algoritmos de entrenamiento.\n",
    "    - **Azure DevOps**: automatiza tareas de compilación y configuración de software.\n",
    "    \n",
    "    Es posible tener un Pipeline de Azure DevOps que ejecute un Pipeline de Azure ML, el cual puede incluir pasos para que entrene un modelo basado en un Pipeline de Scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Concepto de Pipeline\n",
    "    - Es un flujo de trabajo compuesto de tareas de aprendizaje automático.\n",
    "    - Cada tarea se implementa como un paso (secuencial o paralelo).\n",
    "    - Permite construir lógica de flujo sofisticada para organizar operaciones de aprendizaje automático.\n",
    "    \n",
    "2. Ejecución de pasos\n",
    "    - Cada paso se ejecuta en un objetivo de cómputo específico.\n",
    "    - Se pueden combinar diferentes tipos de procesamiento para lograr un objetivo general.\n",
    "\n",
    "3. Ejecución del Pipeline\n",
    "    - Se ejecuta como un experimento.\n",
    "    - Cada paso se ejecuta en su objetivo asignado como parte del experimento.\n",
    "\n",
    "4. Componentes de un Pipeline\n",
    "    - Consta de uno o más pasos que realizan tareas.\n",
    "    - Azure ML admite muchos tipos de pasos:\n",
    "      - `PythonScriptStep`: Ejecuta un script de Python específico.\n",
    "      - `DataTransferStep`: Copia datos entre almacenes mediante Azure Data Factory.\n",
    "      - `DataBrickStep`: Ejecuta un script de notebook o un JAR compilado en un cluster de Databricks.\n",
    "      - `AdlaStep`: Ejecuta un trabajo de SQL en Azure Data Lake Analytics.\n",
    "      - `ParallelRunStep`: Ejecuta un script de Python como una tarea distribuida en múltiples nodos de cómputo.\n",
    "      - Ver la documentación para una lista completa de tipos de pasos compatibles.\n",
    "\n",
    "5. Creación de un Pipeline\n",
    "   - Se requiere definir cada paso primero.\n",
    "   - Luego se crea un pipeline que incluye los pasos.\n",
    "   - La configuración específica de cada paso depende del tipo.\n",
    "     - Ejemplo: definir dos pasos de script de Python para preparar datos y entrenar un modelo.\n",
    "   - Una vez definidos los pasos, se asignan al pipeline y se ejecutan como un experimento.\n",
    "  \n",
    "6. Flujo de trabajo\n",
    "    - Un pipeline suele tener pasos que dependen de la salida de pasos anteriores.\n",
    "      - Ejemplo: un script preprocesando datos (paso 1) usados luego para entrenar un modelo (paso 2).\n",
    "\n",
    "7. Objeto de configuración de conjunto de datos de archivo de salida\n",
    "    - Objeto especial que referencia una ubicación para almacenamiento intermedio de datos.\n",
    "    - Crea una dependencia de datos entre pasos del pipeline.\n",
    "    - Actúa como un almacenamiento intermedio para pasar datos entre pasos.\n",
    "\n",
    "8. Pasando datos entre los pasos\n",
    "   - Se usa el objeto de configuración de conjunto de datos de archivo de salida.\n",
    "   - Debes definir un objeto con nombre que referencie una ubicación en un almacén de datos.\n",
    "   - Si no se especifica un almacén, se usa el predeterminado.\n",
    "   - Pasa el objeto como argumento de script en pasos que ejecutan scripts.\n",
    "   - Incluye código en esos scripts para escribir o leer datos del objeto de argumento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. OutputFileDatasetConfig pasos entre Inputs y Outputs\n",
    "\n",
    "Para utilizar un objeto `OutputFileDatasetConfig` para pasar datos entre los pasos, se debe:\n",
    "\n",
    "1. Definir un objeto `OutputFileDatasetConfig` con un nombre que haga referencia a una ubicación en un almacén de datos. Si no se especifica un almacén de datos explicitamente, se utilizará el almacén de datos predeterminado.\n",
    "2. Pasar el objeto `OutputFileDatasetConfig` como argumento en los pasos que ejecutan scripts.\n",
    "3. Incluya código en esos scripts para escribir en el argumento `OutputFileDatasetConfig` como salida o leerlo como entrada.\n",
    "\n",
    "Por ejemplo, el siguiente código define un objeto `OutputFileDatasetConfig` que para los datos preprocesados que deben pasarse entre los pasos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "from azureml.pipeline.steps import PythonScriptStep, EstimatorStep\n",
    "\n",
    "raw_ds = Dataset.get_by_name(ws, 'raw_dataset')\n",
    "\n",
    "data_store = ws.get_default_datastore()                                                        \n",
    "prepped_data = OutputFileDatasetConfig('prepped')                                               # Creamos un objeto OutputFileDatasetConfig con la ubicación de salida\n",
    "\n",
    "\n",
    "step1 = PythonScriptStep(name = 'prepare data',                                                 # Creamos el primer paso del pipeline para ejecutar data_prep.py\n",
    "                         source_directory = 'scripts',\n",
    "                         script_name = 'data_prep.py',\n",
    "                         compute_target = 'aml-cluster',\n",
    "                         \n",
    "                         # Los argumentos del script incluyen PipelineData\n",
    "                         arguments = ['--raw-ds', raw_ds.as_named_input('raw_data'),            # Pasamos el conjunto de datos como argumento\n",
    "                                      '--out_folder', prepped_data])                            # Pasamos la ubicación de salida como argumento \n",
    "\n",
    "\n",
    "step2 = PythonScriptStep(name = 'train model',                                                  # Creamos el segundo paso del pipeline para ejecutar train_model.py                                                             \n",
    "                         source_directory = 'scripts',\n",
    "                         script_name = 'train_model.py',\n",
    "                         compute_target = 'aml-cluster',\n",
    "                         \n",
    "                         # Pasamos como argumento del script\n",
    "                         arguments=['--training-data', prepped_data.as_input()])                # Pasamos la ubicación de salida como argumento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los propios scripts, se puede obtener una referencia al objeto `OutputFileDatasetConfig` desde el argumento, y utilizarlo como una carpeta local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "\n",
    "from azureml.core import Run\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "parser = argparse.ArgumentParser()                                                              # Creamos un objeto ArgumentParser para manejar los argumentos del script\n",
    "parser.add_argument('--raw-ds', type=str, dest='raw_dataset_id')\n",
    "parser.add_argument('--out_folder', type=str, dest='folder')\n",
    "args = parser.parse_args()\n",
    "output_folder = args.folder\n",
    "\n",
    "raw_df = run.input_datasets['raw_data'].to_pandas_dataframe()                                   # Obtenemos el conjunto de datos por su nombre y lo convertimos a un DataFrame de pandas\n",
    "\n",
    "# code to prep data (in this case, just select specific columns)\n",
    "prepped_df = raw_df[['col1', 'col2', 'col3']]\n",
    "\n",
    "# Save prepped data to the PipelineData location\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "output_path = os.path.join(output_folder, 'prepped_data.csv')\n",
    "prepped_df.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Reutilizar pasos de pipeline\n",
    "\n",
    "Las canalizaciones con varios pasos de larga duración pueden tardar mucho tiempo en completarse. Azure ML incluye algunas características de almacenamiento en caché y reutilización para reducir estos tiempos.\n",
    "\n",
    "- **Gestión de la reutilización de la salida de pasos**\n",
    "  \n",
    "    De forma predeterminada, la salida del paso de una ejecución de pipeline anterior se reutiliza sin volver a ejecutar el paso, siempre que el script, el directorio de origen y otros parámetros del paso no hayan cambiado. La reutilización de pasos puede reducir el tiempo que se tarda en ejecutar cada pipeline, pero tambien puede dar lugar a resultados obsoletos cuando no se han tenido en cuenta los cambios en los datos posteriores.\n",
    "\n",
    "    Para controlar la reutilización de un paso individual, puede establecer el  parámetro `allow_reuse` en la configuración del paso, de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step1 = PythonScriptStep(name = 'prepare data',\n",
    "                         source_directory = 'scripts',\n",
    "                         script_name = 'data_prep.py',\n",
    "                         compute_target = 'aml-cluster',\n",
    "                         runconfig = run_config,\n",
    "                         inputs=[raw_ds.as_named_input('raw_data')],\n",
    "                         outputs=[prepped_data],\n",
    "                         arguments = ['--folder', prepped_data]),\n",
    "                         # Disable step reuse\n",
    "                         allow_reuse = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Forzar la ejecución de todos los pasos**\n",
    "    Cuando tiene varios pasos, podemos forzar la ejecución de todos ellos independientemente de la configuración de reutilización individual, estableciendo el parámetro `regenerate_outputs` al enviar el experimento de pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run = experiment.submit(train_pipeline, regenerate_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Publicar pipelines\n",
    "\n",
    "Después de crear una canalización, puede publicarla para crear un punto de conexión REST a través del cual se pueda ejecutar la canalización a petición.\n",
    "\n",
    "- **Publicación de una canalización**\n",
    "\n",
    "    Para publicar una canalización, puede llamar a su  método de publicación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline.publish(name='training_pipeline',\n",
    "                                          description='Model training pipeline',\n",
    "                                          version='1.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como alternativa, puede llamar al método de publicación en una ejecución correcta de la canalización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most recent run of the pipeline\n",
    "pipeline_experiment = ws.experiments.get('training-pipeline')\n",
    "run = list(pipeline_experiment.get_runs())[0]\n",
    "\n",
    "# Publish the pipeline from the run\n",
    "published_pipeline = run.publish_pipeline(name='training_pipeline',\n",
    "                                          description='Model training pipeline',\n",
    "                                          version='1.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez publicada la canalización, puede verla en Azure Machine Learning Studio. También puede determinar el URI de su punto de conexión de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_endpoint = published_pipeline.endpoint\n",
    "print(rest_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Uso de una canalización publicada**\n",
    "\n",
    "    Para iniciar un punto de conexión publicado, realice una solicitud HTTP a su punto de conexión REST, pasando un encabezado de autorización con un token para una entidad de servicio con permiso para ejecutar la canalización y una carga JSON que especifique el nombre del experimento. La canalización se ejecuta de forma asincrónica, por lo que la respuesta de una llamada REST correcta incluye el identificador de ejecución. Puede usarlo para realizar un seguimiento de la ejecución en Azure Machine Learning Studio.\n",
    "\n",
    "Por ejemplo, el siguiente código de Python realiza una solicitud REST para ejecutar una canalización y muestra el identificador de ejecución devuelto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post(rest_endpoint,\n",
    "                         headers=auth_header,\n",
    "                         json={\"ExperimentName\": \"run_training_pipeline\"})\n",
    "run_id = response.json()[\"Id\"]\n",
    "print(run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Uso de parámetros de pipeline\n",
    "\n",
    "Podemos aumentar la flexibilidad de una canalización definiendo parámetros.\n",
    "\n",
    "- **Definición de parámetros para una canalización**\n",
    "\n",
    "    Para definir parámetros para una canalización, cree un  objeto PipelineParameter para cada parámetro y especifique cada parámetro en al menos un paso.\n",
    "\n",
    "    NOTA: Debemos definir los parámetros de una canalización antes de publicarla!\n",
    "\n",
    "Por ejemplo, puede usar el siguiente código para incluir un parámetro para una tasa de regularización en el script utilizado por un estimador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core.graph import PipelineParameter\n",
    "\n",
    "reg_param = PipelineParameter(name='reg_rate', default_value=0.01)\n",
    "\n",
    "...\n",
    "\n",
    "step2 = PythonScriptStep(name = 'train model',\n",
    "                         source_directory = 'scripts',\n",
    "                         script_name = 'data_prep.py',\n",
    "                         compute_target = 'aml-cluster',\n",
    "                         # Pass parameter as script argument\n",
    "                         arguments=['--in_folder', prepped_data,\n",
    "                                    '--reg', reg_param],\n",
    "                         inputs=[prepped_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Ejecución de una canalización con un parámetro**\n",
    "\n",
    "Después de publicar una canalización con parámetros, puede pasar valores de parámetro en la carga JSON para la interfaz REST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(rest_endpoint,\n",
    "                         headers=auth_header,\n",
    "                         json={\"ExperimentName\": \"run_training_pipeline\",\n",
    "                               \"ParameterAssignments\": {\"reg_rate\": 0.1}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Programar Pipelines\n",
    "\n",
    "Después de publicar una canalización, puede iniciarla a petición a través de su punto de conexión REST, o puede hacer que la canalización se ejecute automáticamente en función de una programación periódica o en respuesta a actualizaciones de datos.\n",
    "\n",
    "- **Programación de una canalización para intervalos periódicos**\n",
    "\n",
    "    Para programar una canalización para que se ejecute a intervalos periódicos, debe definir un ScheduleRecurrence que determine la frecuencia de ejecución y usarlo para crear un Schedule.\n",
    "\n",
    "Por ejemplo, el código siguiente programa una ejecución diaria de una canalización publicada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import ScheduleRecurrence, Schedule\n",
    "\n",
    "daily = ScheduleRecurrence(frequency='Day', interval=1)\n",
    "pipeline_schedule = Schedule.create(ws, name='Daily Training',\n",
    "                                        description='trains model every day',\n",
    "                                        pipeline_id=published_pipeline.id,\n",
    "                                        experiment_name='Training_Pipeline',\n",
    "                                        recurrence=daily)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Desencadenar una ejecución de canalización en los cambios de datos**\n",
    "\n",
    "    Para programar una canalización para que se ejecute cada vez que cambien los datos, debe crear una programación que supervise una ruta de acceso especificada en un almacén de datos, como se muestra a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "from azureml.pipeline.core import Schedule\n",
    "\n",
    "training_datastore = Datastore(workspace=ws, name='blob_data')\n",
    "pipeline_schedule = Schedule.create(ws, name='Reactive Training',\n",
    "                                    description='trains model on data change',\n",
    "                                    pipeline_id=published_pipeline_id,\n",
    "                                    experiment_name='Training_Pipeline',\n",
    "                                    datastore=training_datastore,\n",
    "                                    path_on_datastore='data/training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementacion de servicios de aprendizaje automatico en tiempo real con Azure ML\n",
    "\n",
    "- **Inferencia**: Uso de un modelo entrenado para predecir etiquetas para datos nuevos (no usados durante el entrenamiento).\n",
    "\n",
    "- **Servicio de Inferencia en Tiempo Real**: Permite a las aplicaciones solicitar predicciones inmediatas al modelo para datos individuales o pequeños conjuntos.\n",
    "\n",
    "Despliegue del Modelo:\n",
    "\n",
    "- **Contenedorizado en AKS (Azure Kubernetes Services)**: Plataforma de orquestación de contenedores para implementar y administrar aplicaciones en contenedores.\n",
    "\n",
    "- **Servicio de Inferencia**: Punto de acceso para que las aplicaciones consuman el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un modelo puede implementarse como un servicio web en tiempo real en varios destinos, incluyendo localmente, Azure ML, ACI, AKS, una función de Azure o un módulo IoT. Azure ML contenedores para la implementación, empaquetando el modelo y el código en una imagen que se puede implementar en un contenedor en el destino seleccionado.\n",
    "\n",
    "Nota\n",
    "\n",
    "    La implementación en un servicio local, una instancia informática o una ACI es una buena opción para las pruebas y el desarrollo. Para producción, se debe implementar en un destino que satisfaga las necesidades específicas de rendimiento, escalabilidad y seguridad de la arquitectura de la aplicación.\n",
    "\n",
    "Para implementar un modelo como un servicio de inferencia en tiempo real, debe realizar las siguientes tareas:\n",
    "\n",
    "#### 1. Registro de un modelo entrenado\n",
    "Después de entrenar correctamente un modelo, debe registrarlo en el área de trabajo de Azure ML. Su servicio en tiempo real podrá cargar el modelo cuando sea necesario.\n",
    "\n",
    "Para registrar un modelo a partir de un archivo local, puede utilizar el método `register` del  objeto Model como se muestra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuando tienes un modelo que se ha entrenado fuera de Azure ML y quieres registrar el modelo en tu espacio de trabajo.\n",
    "\n",
    "from azureml.core import Model\n",
    "\n",
    "classification_model = Model.register(workspace=ws,                         # Registramos el modelo en el espacio de trabajo\n",
    "                       model_name='classification_model',\n",
    "                       model_path='model.pkl',                              # ruta local\n",
    "                       description='A classification model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternativamente, si tenemos referencia al `run` utilizado para entrenar el modelo, podemos utilizar su método `register_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run en Azure ML se refiere a una ejecución individual de un script de entrenamiento o un pipeline.\n",
    "# Cuando se llama a run.register_model, el modelo se registra junto con metadatos sobre el Run específico, como los parámetros de entrenamiento utilizados. \n",
    "# Esto puede ser útil para rastrear cómo se creó el modelo.\n",
    "\n",
    "run.register_model( model_name='classification_model',                      # Registramos el modelo en el espacio de trabajo\n",
    "                    model_path='outputs/model.pkl',                         # run ruta de salida\n",
    "                    description='A classification model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Definir una configuración de inferencia\n",
    "\n",
    "Los modelos que se implementan como un servicio, constan de:\n",
    "\n",
    "1. Un script para cargar el modelo y devolver predicciones para los datos enviados.\n",
    "2. Un entorno en el que se ejecutará el script.\n",
    "\n",
    "##### Crear un script de entrada\n",
    "\n",
    "Creando la secuencia de comandos de entrada (tambien llamada **secuencia de comandos de puntuación**), para el servicio como un archivo de Python (.py), necesitamos de:\n",
    "\n",
    "   - `init()`: Se llama cuando se inicializa el servicio.\n",
    "   - `run(raw_data)`: Se llama cuando se envían nuevos datos al servicio.\n",
    "\n",
    "Normalmente, se usa la función `init` para cargar el modelo desde el registro de modelos y se usa la función `run` para generar predicciones a partir de los datos de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "from azureml.core.model import Model\n",
    "\n",
    "# llamamos a la función init() para cargar el modelo en la memoria cuando se inicia el contenedor\n",
    "def init():\n",
    "    global model\n",
    "    model_path = Model.get_model_path('classification_model')                   # Obtenemos la ruta del modelo y lo cargamos en memoria\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "\n",
    "# llamamos a la función run() para obtener una predicción para los datos de entrada cuando se realiza una solicitud al servicio web\n",
    "def run(raw_data):\n",
    "    data = np.array(json.loads(raw_data)['data'])                               # Convertimos los datos de entrada en un array de numpy\n",
    "    predictions = model.predict(data)                                           # Realizamos las predicciones\n",
    "    return predictions.tolist()                                                 # Devolvemos las predicciones como una lista serializable en JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Crear un entorno\n",
    "\n",
    "Requerimos de un entorno de Python en el que ejecutar el script de entrada, que se puede configurar mediante el archivo de configuración de Conda. \n",
    "\n",
    "Una manera sencilla de crear este archivo es usar una clase `CondaDependencies` para crear un entorno predeterminado (que incluye el paquete `azureml-defaults` y paquetes de uso común como `numpy` y `pandas`, pero podemos agregar cualquier otro paquete necesario), a continuación, serealizamos el entorno en una cadena y lo guardamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# Creamos un objeto CondaDependencies y agregamos las dependencias necesarias\n",
    "myenv = CondaDependencies()\n",
    "myenv.add_conda_package(\"scikit-learn\")\n",
    "\n",
    "# Guardamos el archivo de especificación de entorno\n",
    "env_file = 'service_files/env.yml'\n",
    "with open(env_file,\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())\n",
    "print(\"Saved dependency info in\", env_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combinar el script y el entorno en un InferenceConfig\n",
    "\n",
    "Después de crear el script de entrada y el archivo de configuración del entorno, podemos combinarlos en un `InferenceConfig` para el servicio de la siguiente manera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "# Creamos un objeto InferenceConfig con la configuración necesaria\n",
    "classifier_inference_config = InferenceConfig(runtime= \"python\",\n",
    "                                              source_directory = 'service_files',\n",
    "                                              entry_script=\"score.py\",\n",
    "                                              conda_file=\"env.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Definir una configuración de implementación\n",
    "\n",
    "Con el script y el archivo de entorno, ahora debemos configurar el proceso en el que se implementará el servicio. \n",
    "\n",
    "Si vamos a realizar la implementación en un clúster de AKS, tenemos que crear el clúster y un destino de proceso para él antes de realizar la implementación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AksCompute\n",
    "\n",
    "# Creamos un objeto AksCompute con la configuración necesaria para el clúster de AKS\n",
    "cluster_name = 'aks-cluster'\n",
    "compute_config = AksCompute.provisioning_configuration(location='eastus')\n",
    "production_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "production_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez creado el destino del proceso, podemos definir la configuración de implementación. Esta configuración establece la especificación del proceso, específica para el destino, para la implementación en contenedores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AksWebservice\n",
    "\n",
    "# Creamos un objeto AksWebservice con la configuración necesaria para el servicio web\n",
    "classifier_deploy_config = AksWebservice.deploy_configuration(cpu_cores = 1,\n",
    "                                                              memory_gb = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para configurar una implementación de ACI, el código es similar. \n",
    "\n",
    "No necesitas crear un destino de proceso de ACI explícitamente y debes usar la clase `deploy_configuration` del espacio de nombres `azureml.core.webservice.AciWebservice`. De manera similar, puedes usar el espacio de nombres `azureml.core.webservice.LocalWebservice` para configurar un servicio local basado en Docker.\n",
    "\n",
    "Si queremos implementar un modelo en una función de Azure, no necesitas crear una configuración de implementación. En su lugar, debes empaquetar el modelo según el tipo de desencadenador de función que quieras usar. Esta funcionalidad está en versión preliminar en el momento de escribir este artículo. Para obtener más información, consulta la sección [“Implementación de un modelo de aprendizaje automático en Azure Functions”](https://learn.microsoft.com/es-es/azure/machine-learning/how-to-deploy-online-endpoints?view=azureml-api-2&tabs=azure-cli) en la documentación de Azure ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implementar el modelo\n",
    "Una vez preparada toda la configuración, podemos implementar el modelo. La forma más fácil de hacerlo es llamar al método deploy de la clase `Model`, de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con '/bin/python3' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "model = ws.models['classification_model']                                       # Obtenemos el modelo por su nombre\n",
    "service = Model.deploy(workspace=ws,                                            # Desplegamos el servicio web en el clúster de AKS\n",
    "                       name = 'classifier-service',\n",
    "                       models = [model],\n",
    "                       inference_config = classifier_inference_config,\n",
    "                       deployment_config = classifier_deploy_config,\n",
    "                       deployment_target = production_cluster)\n",
    "service.wait_for_deployment(show_output = True)                                 # Esperamos a que se complete el despliegue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de ACI o servicios locales, puede omitir el parámetro `deployment_target` (o establecerlo en **None**).\n",
    "\n",
    "[Más into sobre la implementación de modelos con Azure Machine Learning](https://learn.microsoft.com/es-es/azure/machine-learning/how-to-deploy-online-endpoints?view=azureml-api-2&tabs=azure-cli)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Consumo de un servicio de inferencia en tiempo real\n",
    "\n",
    "Después de implementar el servicio en tiempo real, puede consumirlo desde aplicaciones cliente para predecir etiquetas para nuevos casos de datos.\n",
    "\n",
    "##### Uso del SDK de Azure Machine Learning\n",
    "\n",
    "Para realizar pruebas, puede usar el SDK de Azure ML para llamar a un servicio web a través del método run de un objeto `WebService` que haga referencia al servicio implementado. Normalmente, los datos se envían al método `run` en formato JSON con la siguiente estructura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"data\":[\n",
    "      [0.1,2.3,4.1,2.0],  // 1st case\n",
    "      [0.2,1.8,3.9,2.1],  // 2nd case\n",
    "      ...\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La respuesta del método `run` sera una colección JSON con una predicción para cada caso que se envió en los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En este ejemplo se llama a un servicio y se muestra la respuesta\n",
    "\n",
    "import json\n",
    "\n",
    "# matriz de datos nuevos\n",
    "x_new = [[0.1,2.3,4.1,2.0],\n",
    "         [0.2,1.8,3.9,2.1]]\n",
    "\n",
    "\n",
    "json_data = json.dumps({\"data\": x_new})                     # Convertimos la matriz en un formato JSON serializable\n",
    "\n",
    "response = service.run(input_data = json_data)              # Llamamos al servicio web con los datos de entrada\n",
    "\n",
    "predictions = json.loads(response)                          # Convertimos la respuesta en un objeto JSON\n",
    "\n",
    "for i in range(len(x_new)):                                 # Mostramos las predicciones para cada conjunto de datos\n",
    "    print (x_new[i], predictions[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Uso de un punto de conexión REST \n",
    "\n",
    "En producción, la mayoría de las aplicaciones cliente no incluirán el SDK de Azure ML y consumirán el servicio a través de su interfaz REST. \n",
    "\n",
    "Podemos determinar el punto de conexión de un servicio implementado en Azure ML Studio o recuperar la propiedad `scoring_uri` del objeto Webservice en el SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En este ejemplo se muestra cómo obtener la dirección URL del servicio web\n",
    "\n",
    "endpoint = service.scoring_uri\n",
    "print(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el punto de conexión conocido, puede usar una solicitud HTTP POST con datos JSON para llamar al servicio. En el siguiente ejemplo se muestra cómo hacerlo con Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "x_new = [[0.1,2.3,4.1,2.0],\n",
    "         [0.2,1.8,3.9,2.1]]\n",
    "\n",
    "json_data = json.dumps({\"data\": x_new})                         # Convertimos la matriz en un formato JSON serializable\n",
    "\n",
    "request_headers = { 'Content-Type':'application/json' }         # Set the content type in the request headers\n",
    "\n",
    "response = requests.post(url = endpoint,                        # Realizamos la solicitud POST al servicio web\n",
    "                         data = json_data,\n",
    "                         headers = request_headers)\n",
    "\n",
    "predictions = json.loads(response.json())                       # Convertimos la respuesta en un objeto JSON\n",
    "\n",
    "for i in range(len(x_new)):                                     # Imprimimos las predicciones para cada conjunto de datos\n",
    "    print ((x_new[i]), predictions[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Autenticación\n",
    "\n",
    "En producción, es probable que queramos restringir el acceso a los servicios mediante la aplicación de autenticación. Hay dos tipos de autenticación que se pueden aplicar:\n",
    "- **Clave**: las solicitudes se autentican especificando la clave asociada al servicio.\n",
    "- **Token**: se autentican proporcionando un token web JSON (JWT).\n",
    "\n",
    "De forma predeterminada, la autenticación está deshabilitada para los servicios ACI y se establece en la autenticación basada en claves para los servicios de AKS (para los que las claves principal y secundaria se generan automáticamente). Opcionalmente, puede configurar un servicio de AKS para usar la autenticación basada en tokens (que no es compatible con los servicios ACI).\n",
    "\n",
    "Suponiendo que tenemos una sesión autenticada establecida con el área de trabajo, podemos recuperar las claves del servicio mediante el método `get_keys` del objeto WebService asociado al servicio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_key, secondary_key = service.get_keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de la autenticación basada en tokens, la aplicación cliente debe usar la autenticación de entidad de servicio para comprobar su identidad a través de Azure Active Directory (Azure AD) y llamar al  método `get_token` del servicio para recuperar un token de tiempo limitado.\n",
    "\n",
    "Para realizar una llamada autenticada al punto de conexión REST del servicio, debemos incluir la clave o el token en el encabezado de la solicitud de la siguiente manera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "x_new = [[0.1,2.3,4.1,2.0],\n",
    "         [0.2,1.8,3.9,2.1]]\n",
    "\n",
    "json_data = json.dumps({\"data\": x_new})\n",
    "\n",
    "request_headers = { \"Content-Type\":\"application/json\",                      # incluimos la clave de autenticación en la solicitud\n",
    "                    \"Authorization\":\"Bearer \" + key_or_token }\n",
    "\n",
    "response = requests.post(url = endpoint,                                    # llama al servicio web con los datos de entrada\n",
    "                         data = json_data,\n",
    "                         headers = request_headers)\n",
    "\n",
    "predictions = json.loads(response.json())\n",
    "\n",
    "for i in range(len(x_new)):\n",
    "    print ((x_new[i]), predictions[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Implementación de un modelo como servicio web en tiempo real\n",
    "\n",
    "Destinos de proceso\n",
    "- Proceso local\n",
    "- Instancia de proceso de Azure Machine Learning\n",
    "- Instancia de Azure Container Instance (ACI)\n",
    "- Clúster de Azure Kubernetes Service (AKS)\n",
    "- Función de Azure\n",
    "- Módulo de Internet de las cosas (IoT)\n",
    "\n",
    "Mecanismo de implementación\n",
    "- Contenedores\n",
    "- Empaquetado del modelo y el código como una imagen\n",
    "- Implementación en un contenedor en el destino elegido\n",
    "\n",
    "Consideraciones:\n",
    "- Pruebas y desarrollo: servicio local, instancia informática o ACI\n",
    "- Producción: destino que cumpla con los requisitos de rendimiento, escalabilidad y seguridad de la aplicación\n",
    "\n",
    "Tareas para implementar un servicio de inferencia en tiempo real:\n",
    "1. **Registrar el modelo y el entorno**: Almacenar el modelo y su entorno de ejecución en Azure Machine Learning.\n",
    "2. **Crear una configuración de inferencia**: Especificar cómo se ejecuta el modelo en tiempo real.\n",
    "3. **Crear una configuración de destino**: Seleccionar el destino de proceso y las opciones de implementación.\n",
    "4. **Implementar el servicio**: Implementar el modelo como un servicio web en el destino elegido.\n",
    "5. **Probar el servicio**: Enviar solicitudes de prueba al servicio y verificar las predicciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
