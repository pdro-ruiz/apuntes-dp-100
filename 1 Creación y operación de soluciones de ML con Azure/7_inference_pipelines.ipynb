{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación de pipelines de inferencia por lotes\n",
    "\n",
    "La inferencia por lotes se utiliza en escenarios de producción con tareas de larga duración y grandes volúmenes de datos\n",
    "\n",
    "**Inferencia por lotes**: Aplicación de un modelo predictivo a múltiples casos de forma asincrónica, generando resultados en un archivo o base de datos.\n",
    "\n",
    "**Azure Machine Learning**: Plataforma para crear soluciones de inferencia por lotes.\n",
    "\n",
    "Pipeline de inferencia por lotes:\n",
    "- Lee datos de entrada.\n",
    "- Carga un modelo registrado.\n",
    "- Predice etiquetas.\n",
    "- Escribe los resultados como salida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para crear una canalización de inferencia por lotes, Hay que realizar los siguiente:\n",
    "\n",
    "**1. Registrar un modelo**\n",
    "\n",
    "Para utilizar un modelo ya entrenado en un pipeline de inferencia por lotes, es necesario registrarlo en tu espacio de trabajo de Azure ML.\n",
    "\n",
    "Si deseas registrar un modelo a partir de un archivo local, puedes hacer uso del método `register` del objeto Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Model\n",
    "\n",
    "classification_model = Model.register(workspace=your_workspace,                         # registramos el modelo en el workspace\n",
    "                                      model_name='classification_model',\n",
    "                                      model_path='model.pkl',                           # ruta local del modelo\n",
    "                                      description='A classification model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como alternativa, si tienes una referencia a la ejecución que se utilizó para entrenar el modelo, puedes hacer uso del método `register_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.register_model( model_name='classification_model',\n",
    "                    model_path='outputs/model.pkl', # run outputs path\n",
    "                    description='A classification model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Crear un guión de puntuación**\n",
    "\n",
    "El servicio de inferencia por lotes necesita un script de puntuación para cargar el modelo y usarlo para generar nuevas predicciones. Este script debe contener dos funciones:\n",
    "\n",
    "- `init()`: Esta función se invoca cuando se inicializa el pipeline.\n",
    "- `run(mini_batch)`: Esta función se invoca para cada lote de datos que se va a procesar.\n",
    "\n",
    "Por lo general, la función `init` se utiliza para cargar el modelo desde el registro de modelos, y la función `run` se utiliza para generar predicciones a partir de cada lote de datos y devolver los resultados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from azureml.core import Model\n",
    "import joblib\n",
    "\n",
    "def init():\n",
    "    # Runs when the pipeline step is initialized\n",
    "    global model\n",
    "\n",
    "    # load the model\n",
    "    model_path = Model.get_model_path('classification_model')\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "def run(mini_batch):\n",
    "    # This runs for each batch\n",
    "    resultList = []\n",
    "\n",
    "    # process each file in the batch\n",
    "    for f in mini_batch:\n",
    "        # Read comma-delimited data into an array\n",
    "        data = np.genfromtxt(f, delimiter=',')\n",
    "        # Reshape into a 2-dimensional array for model input\n",
    "        prediction = model.predict(data.reshape(1, -1))\n",
    "        # Append prediction to results\n",
    "        resultList.append(\"{}: {}\".format(os.path.basename(f), prediction[0]))\n",
    "    return resultList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Creación de una canalización con un ParallelRunStep**\n",
    "\n",
    "Azure Machine Learning ofrece un tipo específico de paso de canalización, llamado `ParallelRunStep`, para realizar inferencias por lotes en paralelo. Esta clase permite leer lotes de archivos de un conjunto de datos de tipo `File` y escribir la salida del procesamiento en un `OutputFileDatasetConfig`.\n",
    "\n",
    "Además, puedes configurar la opción `output_action` del paso a “append_row”. Esto asegura que todas las instancias del paso que se ejecutan en paralelo recopilen sus resultados en un único archivo de salida llamado *parallel_run_step.txt*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import ParallelRunConfig, ParallelRunStep\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "# Get the batch dataset for input\n",
    "batch_data_set = ws.datasets['batch-data']\n",
    "\n",
    "# Set the output location\n",
    "default_ds = ws.get_default_datastore()\n",
    "output_dir = OutputFileDatasetConfig(name='inferences')\n",
    "\n",
    "# Define the parallel run step step configuration\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory='batch_scripts',\n",
    "    entry_script=\"batch_scoring_script.py\",\n",
    "    mini_batch_size=\"5\",\n",
    "    error_threshold=10,\n",
    "    output_action=\"append_row\",\n",
    "    environment=batch_env,\n",
    "    compute_target=aml_cluster,\n",
    "    node_count=4)\n",
    "\n",
    "# Create the parallel run step\n",
    "parallelrun_step = ParallelRunStep(\n",
    "    name='batch-score',\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[batch_data_set.as_named_input('batch_data')],\n",
    "    output=output_dir,\n",
    "    arguments=[],\n",
    "    allow_reuse=True\n",
    ")\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(workspace=ws, steps=[parallelrun_step])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Ejecute la canalización y recupere el resultado del paso**\n",
    "\n",
    "Una vez que hayas definido la canalización, puedes ejecutarla y esperar a que finalice. Posteriormente, puedes obtener el archivo *parallel_run_step.txt* de la salida del paso para revisar los resultados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "# Run the pipeline as an experiment\n",
    "pipeline_run = Experiment(ws, 'batch_prediction_pipeline').submit(pipeline)\n",
    "pipeline_run.wait_for_completion(show_output=True)\n",
    "\n",
    "# Get the outputs from the first (and only) step\n",
    "prediction_run = next(pipeline_run.get_children())\n",
    "prediction_output = prediction_run.get_output_data('inferences')\n",
    "prediction_output.download(local_path='results')\n",
    "\n",
    "# Find the parallel_run_step.txt file\n",
    "for root, dirs, files in os.walk('results'):\n",
    "    for file in files:\n",
    "        if file.endswith('parallel_run_step.txt'):\n",
    "            result_file = os.path.join(root,file)\n",
    "\n",
    "# Load and display the results\n",
    "df = pd.read_csv(result_file, delimiter=\":\", header=None)\n",
    "df.columns = [\"File\", \"Prediction\"]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Publicar una canalización de inferencia por lotes\n",
    "\n",
    "Podemos publicar un Pipeline de inferencia por lotes como un servicio REST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline_run.publish_pipeline(name='Batch_Prediction_Pipeline',\n",
    "                                                   description='Batch pipeline',\n",
    "                                                   version='1.0')\n",
    "rest_endpoint = published_pipeline.endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez publicado, podemos usar el punto de conexión de servicio para iniciar el trabajo de inferencia por lotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post(rest_endpoint,\n",
    "                         headers=auth_header,\n",
    "                         json={\"ExperimentName\": \"Batch_Prediction\"})\n",
    "run_id = response.json()[\"Id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos programar la canalización publicada para que se ejecute automáticamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import ScheduleRecurrence, Schedule\n",
    "\n",
    "weekly = ScheduleRecurrence(frequency='Week', interval=1)\n",
    "pipeline_schedule = Schedule.create(ws, name='Weekly Predictions',\n",
    "                                        description='batch inferencing',\n",
    "                                        pipeline_id=published_pipeline.id,\n",
    "                                        experiment_name='Batch_Prediction',\n",
    "                                        recurrence=weekly)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
