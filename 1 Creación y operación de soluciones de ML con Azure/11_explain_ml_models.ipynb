{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretación de Modelos de Aprendizaje Automático\n",
    "\n",
    "El aprendizaje automático impacta cada vez más en decisiones cruciales de salud, seguridad, economía y otras áreas.\n",
    "Comprender cómo funcionan los modelos de aprendizaje automático y explicar su razonamiento es fundamental.\n",
    "\n",
    "A mayor interpretabilidad, mayor confianza y transparencia en las decisiones basadas en modelos de aprendizaje automático.\n",
    "\n",
    "Dificultad de la explicación:\n",
    "- La diversidad de algoritmos y la naturaleza del aprendizaje automático dificultan la explicación de los modelos.\n",
    "\n",
    "Solución:\n",
    "- La interpretabilidad de modelos ayuda a explicar las predicciones de los modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importancia de las Características en los Modelos de Aprendizaje Automático\n",
    "\n",
    "Comprender la importancia de las características ayuda a interpretar cómo los modelos de aprendizaje automático hacen predicciones para casos individuales y para el conjunto de datos en general.\n",
    "\n",
    "**Explicando Modelos**:\n",
    "- Los \"explainers\" usan técnicas estadísticas para calcular la importancia de las características en un modelo de aprendizaje automático.\n",
    "- Esto permite cuantificar la influencia relativa de cada característica en la predicción de etiquetas.\n",
    "\n",
    "**Importancia Global**:\n",
    "- Mide la importancia general de cada característica en todo el conjunto de datos de prueba.\n",
    "- Proporciona una comparación general de cómo cada característica influencia la predicción.\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "    Un modelo predice el riesgo de impago de préstamos basado en ingresos, monto del préstamo, edad y estado civil.\n",
    "    La importancia global podría ser: ingresos (0.98), monto del préstamo (0.67), edad (0.54) y estado civil (0.32).\n",
    "\n",
    "**Importancia Local**:\n",
    "- Mide la influencia de cada valor de característica para una predicción individual.\n",
    "- Se puede usar para entender qué factores influyen en la predicción para una instancia específica.\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "    Se aprueba el préstamo de Sam.\n",
    "    El explainer indica que el monto del préstamo y luego el ingreso son los más importantes para aprobar el préstamo de Sam.\n",
    "    Esto difiere de la importancia global, ya que Sam podría tener un ingreso inferior al promedio pero un monto de préstamo inusualmente bajo.\n",
    "\n",
    "\n",
    "**Modelos Multi-Clase**:\n",
    "- La importancia local se calcula para cada clase posible.\n",
    "- La suma de las importancias locales para todas las clases de una característica siempre es cero.\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "    Un modelo predice la especie de pingüino basado en el largo del pico, ancho del pico, largo de la aleta y peso.\n",
    "    La importancia local de la longitud de la aleta para un pingüino específico podría ser: 0.5 (clase 0), 0.3 (clase 1) y -0.8 (clase 2).\n",
    "\n",
    "**Modelos de Regresión**:\n",
    "- No tienen clases, por lo que la importancia local indica la influencia de cada característica en la etiqueta o escala predicha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uso de Explainers\n",
    "\n",
    "Se necesita el paquete `azureml-interpret` para crear explicadores.\n",
    "\n",
    "##### Tipos de Explicadores\n",
    "- **MimicExplainer**: Crea un modelo sustituto global que aproxima al modelo original y permite generar explicaciones.\n",
    "  - Requiere la misma arquitectura que el modelo original (ej. lineal, basado en árboles).\n",
    "- **TabularExplainer**: Envuelve diferentes algoritmos de explicación y elige automáticamente el más adecuado para la arquitectura del modelo.\n",
    "- **PFIExplainer**: Analiza la importancia de las características barajando sus valores y midiendo el impacto en la predicción.\n",
    "\n",
    "##### Obtención de Importancia de Características\n",
    "\n",
    "- **Global**:\n",
    "  - Se usa `explained_global` y `get_feature_importance_dict` del explicador para obtener un diccionario con la importancia global de cada característica.\n",
    "  - El código es igual para **MimicExplainer** y **TabularExplainer**.\n",
    "  - **PFIExplainer** requiere las etiquetas reales de las características de prueba.\n",
    "  \n",
    "- **Local**:\n",
    "  - Se usa `explained_local` del explicador para especificar los casos a explicar.\n",
    "  - Se utilizan `get_ranked_local_names` y `get_ranked_local_values` para obtener nombres y valores de importancia ordenados.\n",
    "  - El código es igual para **MimicExplainer** y **TabularExplainer**.\n",
    "  - **PFIExplainer** no soporta explicaciones de importancia local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creación de explains\n",
    "\n",
    "Cuando se utiliza un estimador o un script para entrenar un modelo en un experimento de Azure ML, es posible crear un explicador y cargar la explicación generada durante la ejecución para posteriormente analizarlo.\n",
    "\n",
    "##### 1.Creación de una explicación en el script del experimento\n",
    "\n",
    "Para crear una explicación en el script del experimento, asegúrese de que los paquetes `azureml-interpret` y `azureml-contrib-interpret` estén instalados en el entorno de ejecución. A continuación, puede utilizarlos para generar una explicación a partir del modelo entrenado y cargarla en las salidas de la ejecución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar y cargar una explicación del modelo en un script de experimento\n",
    "\n",
    "from azureml.core.run import Run\n",
    "from azureml.contrib.interpret.explanation.explanation_client import ExplanationClient\n",
    "from interpret.ext.blackbox import TabularExplainer\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "# Codigo de entrenamiento del modelo\n",
    "\n",
    "# crear un explicador\n",
    "explainer = TabularExplainer(model, X_train, features=features, classes=labels)\n",
    "explanation = explainer.explain_global(X_test)\n",
    "\n",
    "# Get an Explanation Client and upload the explanation\n",
    "explain_client = ExplanationClient.from_run(run)\n",
    "explain_client.upload_model_explanation(explanation, comment='Tabular Explanation')             # usamos\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.Visualización de la explicación\n",
    "\n",
    "Para poder ver la explicaciones que se han creado para el modelo en la pestaña **Explicaciones** de la ejecución en Azure ML Studio.\n",
    "\n",
    "También puede usar el objeto `ExplanationClient` para descargar la explicación en Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.interpret.explanation.explanation_client import ExplanationClient\n",
    "\n",
    "client = ExplanationClient.from_run_id(workspace=ws,\n",
    "                                       experiment_name=experiment.experiment_name, \n",
    "                                       run_id=run.id)\n",
    "explanation = client.download_model_explanation()\n",
    "feature_importances = explanation.get_feature_importance_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizaciones de Importancia de Características en Azure Machine Learning Studio\n",
    "\n",
    "Las visualizaciones ayudan a explorar la *influencia relativa* de cada característica en las predicciones del modelo.\n",
    "\n",
    "- Visualizaciones:\n",
    "  - Importancia global:\n",
    "    - Muestra la importancia general de cada característica en todo el conjunto de datos de prueba.\n",
    "    - Permite filtrar para ver solo las K características más importantes usando un control deslizante.\n",
    "\n",
    "- Distribución de importancia:\n",
    "  - Muestra la distribución de los valores de importancia individual para cada característica.\n",
    "  - Ofrece diferentes visualizaciones: diagrama de dispersión (swarm plot), diagrama de cajas (box plot) y diagrama de violín (violin plot).\n",
    "  - Seleccionar un punto de datos permite ver la importancia local para esa instancia específica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra: [Model interpretability](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-interpretability?view=azureml-api-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
