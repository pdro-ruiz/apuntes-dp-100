{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "\n",
    "En Azure ML, las tareas se ejecutan como experimentos utilizando recursos informáticos y datos.\n",
    "\n",
    "Para los procesos empresariales de ciencia de datos, se recomienda dividir el proceso en tareas individuales y orquestarlas con Pipelines (secuencias de pasos conectados).\n",
    "\n",
    "**Los Pipelines son clave para implementar una solución MLOps efectiva.**\n",
    "\n",
    "- Aclaración sobre el término \"Pipeline\"\n",
    "    \n",
    "    El término \"Pipeline\" se usa mucho en ML con significados diferentes.\n",
    "\n",
    "    - **Scikit-learn**: enlaza preprocesamiento de datos con algoritmos de entrenamiento.\n",
    "    - **Azure DevOps**: automatiza tareas de compilación y configuración de software.\n",
    "    \n",
    "    Es posible tener un Pipeline de Azure DevOps que ejecute un Pipeline de Azure ML, el cual puede incluir pasos para que entrene un modelo basado en un Pipeline de Scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Concepto de Pipeline\n",
    "    - Es un flujo de trabajo compuesto de tareas de aprendizaje automático.\n",
    "    - Cada tarea se implementa como un paso (secuencial o paralelo).\n",
    "    - Permite construir lógica de flujo sofisticada para organizar operaciones de aprendizaje automático.\n",
    "    \n",
    "2. Ejecución de pasos\n",
    "    - Cada paso se ejecuta en un objetivo de cómputo específico.\n",
    "    - Se pueden combinar diferentes tipos de procesamiento para lograr un objetivo general.\n",
    "\n",
    "3. Ejecución del Pipeline\n",
    "    - Se ejecuta como un experimento.\n",
    "    - Cada paso se ejecuta en su objetivo asignado como parte del experimento.\n",
    "\n",
    "4. Componentes de un Pipeline\n",
    "    - Consta de uno o más pasos que realizan tareas.\n",
    "    - Azure ML admite muchos tipos de pasos:\n",
    "      - `PythonScriptStep`: Ejecuta un script de Python específico.\n",
    "      - `DataTransferStep`: Copia datos entre almacenes mediante Azure Data Factory.\n",
    "      - `DataBrickStep`: Ejecuta un script de notebook o un JAR compilado en un cluster de Databricks.\n",
    "      - `AdlaStep`: Ejecuta un trabajo de SQL en Azure Data Lake Analytics.\n",
    "      - `ParallelRunStep`: Ejecuta un script de Python como una tarea distribuida en múltiples nodos de cómputo.\n",
    "      - Ver la documentación para una lista completa de tipos de pasos compatibles.\n",
    "\n",
    "5. Creación de un Pipeline\n",
    "   - Se requiere definir cada paso primero.\n",
    "   - Luego se crea un pipeline que incluye los pasos.\n",
    "   - La configuración específica de cada paso depende del tipo.\n",
    "     - Ejemplo: definir dos pasos de script de Python para preparar datos y entrenar un modelo.\n",
    "   - Una vez definidos los pasos, se asignan al pipeline y se ejecutan como un experimento.\n",
    "  \n",
    "6. Flujo de trabajo\n",
    "    - Un pipeline suele tener pasos que dependen de la salida de pasos anteriores.\n",
    "      - Ejemplo: un script preprocesando datos (paso 1) usados luego para entrenar un modelo (paso 2).\n",
    "\n",
    "7. Objeto de configuración de conjunto de datos de archivo de salida\n",
    "    - Objeto especial que referencia una ubicación para almacenamiento intermedio de datos.\n",
    "    - Crea una dependencia de datos entre pasos del pipeline.\n",
    "    - Actúa como un almacenamiento intermedio para pasar datos entre pasos.\n",
    "\n",
    "8. Pasando datos entre los pasos\n",
    "   - Se usa el objeto de configuración de conjunto de datos de archivo de salida.\n",
    "   - Debes definir un objeto con nombre que referencie una ubicación en un almacén de datos.\n",
    "   - Si no se especifica un almacén, se usa el predeterminado.\n",
    "   - Pasa el objeto como argumento de script en pasos que ejecutan scripts.\n",
    "   - Incluye código en esos scripts para escribir o leer datos del objeto de argumento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. OutputFileDatasetConfig pasos entre Inputs y Outputs\n",
    "\n",
    "Para utilizar un objeto `OutputFileDatasetConfig` para pasar datos entre los pasos, se debe:\n",
    "\n",
    "1. Definir un objeto `OutputFileDatasetConfig` con un nombre que haga referencia a una ubicación en un almacén de datos. Si no se especifica un almacén de datos explicitamente, se utilizará el almacén de datos predeterminado.\n",
    "2. Pasar el objeto `OutputFileDatasetConfig` como argumento en los pasos que ejecutan scripts.\n",
    "3. Incluya código en esos scripts para escribir en el argumento `OutputFileDatasetConfig` como salida o leerlo como entrada.\n",
    "\n",
    "Por ejemplo, el siguiente código define un objeto `OutputFileDatasetConfig` que para los datos preprocesados que deben pasarse entre los pasos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "from azureml.pipeline.steps import PythonScriptStep, EstimatorStep\n",
    "\n",
    "raw_ds = Dataset.get_by_name(ws, 'raw_dataset')\n",
    "\n",
    "data_store = ws.get_default_datastore()                                                        \n",
    "prepped_data = OutputFileDatasetConfig('prepped')                                               # Creamos un objeto OutputFileDatasetConfig con la ubicación de salida\n",
    "\n",
    "\n",
    "step1 = PythonScriptStep(name = 'prepare data',                                                 # Creamos el primer paso del pipeline para ejecutar data_prep.py\n",
    "                         source_directory = 'scripts',\n",
    "                         script_name = 'data_prep.py',\n",
    "                         compute_target = 'aml-cluster',\n",
    "                         \n",
    "                         # Los argumentos del script incluyen PipelineData\n",
    "                         arguments = ['--raw-ds', raw_ds.as_named_input('raw_data'),            # Pasamos el conjunto de datos como argumento\n",
    "                                      '--out_folder', prepped_data])                            # Pasamos la ubicación de salida como argumento \n",
    "\n",
    "\n",
    "step2 = PythonScriptStep(name = 'train model',                                                  # Creamos el segundo paso del pipeline para ejecutar train_model.py                                                             \n",
    "                         source_directory = 'scripts',\n",
    "                         script_name = 'train_model.py',\n",
    "                         compute_target = 'aml-cluster',\n",
    "                         \n",
    "                         # Pasamos como argumento del script\n",
    "                         arguments=['--training-data', prepped_data.as_input()])                # Pasamos la ubicación de salida como argumento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los propios scripts, se puede obtener una referencia al objeto `OutputFileDatasetConfig` desde el argumento, y utilizarlo como una carpeta local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "\n",
    "from azureml.core import Run\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "parser = argparse.ArgumentParser()                                                              # Creamos un objeto ArgumentParser para manejar los argumentos del script\n",
    "parser.add_argument('--raw-ds', type=str, dest='raw_dataset_id')\n",
    "parser.add_argument('--out_folder', type=str, dest='folder')\n",
    "args = parser.parse_args()\n",
    "output_folder = args.folder\n",
    "\n",
    "raw_df = run.input_datasets['raw_data'].to_pandas_dataframe()                                   # Obtenemos el conjunto de datos por su nombre y lo convertimos a un DataFrame de pandas\n",
    "\n",
    "# code to prep data (in this case, just select specific columns)\n",
    "prepped_df = raw_df[['col1', 'col2', 'col3']]\n",
    "\n",
    "# Save prepped data to the PipelineData location\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "output_path = os.path.join(output_folder, 'prepped_data.csv')\n",
    "prepped_df.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Reutilizar pasos de pipeline\n",
    "\n",
    "Las canalizaciones con varios pasos de larga duración pueden tardar mucho tiempo en completarse. Azure ML incluye algunas características de almacenamiento en caché y reutilización para reducir estos tiempos.\n",
    "\n",
    "- **Gestión de la reutilización de la salida de pasos**\n",
    "  \n",
    "    De forma predeterminada, la salida del paso de una ejecución de pipeline anterior se reutiliza sin volver a ejecutar el paso, siempre que el script, el directorio de origen y otros parámetros del paso no hayan cambiado. La reutilización de pasos puede reducir el tiempo que se tarda en ejecutar cada pipeline, pero tambien puede dar lugar a resultados obsoletos cuando no se han tenido en cuenta los cambios en los datos posteriores.\n",
    "\n",
    "    Para controlar la reutilización de un paso individual, puede establecer el  parámetro `allow_reuse` en la configuración del paso, de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step1 = PythonScriptStep(name = 'prepare data',\n",
    "                         source_directory = 'scripts',\n",
    "                         script_name = 'data_prep.py',\n",
    "                         compute_target = 'aml-cluster',\n",
    "                         runconfig = run_config,\n",
    "                         inputs=[raw_ds.as_named_input('raw_data')],\n",
    "                         outputs=[prepped_data],\n",
    "                         arguments = ['--folder', prepped_data]),\n",
    "                         # Disable step reuse\n",
    "                         allow_reuse = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Forzar la ejecución de todos los pasos**\n",
    "    Cuando tiene varios pasos, podemos forzar la ejecución de todos ellos independientemente de la configuración de reutilización individual, estableciendo el parámetro `regenerate_outputs` al enviar el experimento de pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run = experiment.submit(train_pipeline, regenerate_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Publicar pipelines\n",
    "\n",
    "Después de crear una canalización, puede publicarla para crear un punto de conexión REST a través del cual se pueda ejecutar la canalización a petición.\n",
    "\n",
    "- **Publicación de una canalización**\n",
    "\n",
    "    Para publicar una canalización, puede llamar a su  método de publicación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline.publish(name='training_pipeline',\n",
    "                                          description='Model training pipeline',\n",
    "                                          version='1.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como alternativa, puede llamar al método de publicación en una ejecución correcta de la canalización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most recent run of the pipeline\n",
    "pipeline_experiment = ws.experiments.get('training-pipeline')\n",
    "run = list(pipeline_experiment.get_runs())[0]\n",
    "\n",
    "# Publish the pipeline from the run\n",
    "published_pipeline = run.publish_pipeline(name='training_pipeline',\n",
    "                                          description='Model training pipeline',\n",
    "                                          version='1.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez publicada la canalización, puede verla en Azure Machine Learning Studio. También puede determinar el URI de su punto de conexión de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_endpoint = published_pipeline.endpoint\n",
    "print(rest_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Uso de una canalización publicada**\n",
    "\n",
    "    Para iniciar un punto de conexión publicado, realice una solicitud HTTP a su punto de conexión REST, pasando un encabezado de autorización con un token para una entidad de servicio con permiso para ejecutar la canalización y una carga JSON que especifique el nombre del experimento. La canalización se ejecuta de forma asincrónica, por lo que la respuesta de una llamada REST correcta incluye el identificador de ejecución. Puede usarlo para realizar un seguimiento de la ejecución en Azure Machine Learning Studio.\n",
    "\n",
    "Por ejemplo, el siguiente código de Python realiza una solicitud REST para ejecutar una canalización y muestra el identificador de ejecución devuelto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post(rest_endpoint,\n",
    "                         headers=auth_header,\n",
    "                         json={\"ExperimentName\": \"run_training_pipeline\"})\n",
    "run_id = response.json()[\"Id\"]\n",
    "print(run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Uso de parámetros de pipeline\n",
    "\n",
    "Podemos aumentar la flexibilidad de una canalización definiendo parámetros.\n",
    "\n",
    "- **Definición de parámetros para una canalización**\n",
    "\n",
    "    Para definir parámetros para una canalización, cree un  objeto PipelineParameter para cada parámetro y especifique cada parámetro en al menos un paso.\n",
    "\n",
    "    NOTA: Debemos definir los parámetros de una canalización antes de publicarla!\n",
    "\n",
    "Por ejemplo, puede usar el siguiente código para incluir un parámetro para una tasa de regularización en el script utilizado por un estimador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core.graph import PipelineParameter\n",
    "\n",
    "reg_param = PipelineParameter(name='reg_rate', default_value=0.01)\n",
    "\n",
    "...\n",
    "\n",
    "step2 = PythonScriptStep(name = 'train model',\n",
    "                         source_directory = 'scripts',\n",
    "                         script_name = 'data_prep.py',\n",
    "                         compute_target = 'aml-cluster',\n",
    "                         # Pass parameter as script argument\n",
    "                         arguments=['--in_folder', prepped_data,\n",
    "                                    '--reg', reg_param],\n",
    "                         inputs=[prepped_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Ejecución de una canalización con un parámetro**\n",
    "\n",
    "Después de publicar una canalización con parámetros, puede pasar valores de parámetro en la carga JSON para la interfaz REST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(rest_endpoint,\n",
    "                         headers=auth_header,\n",
    "                         json={\"ExperimentName\": \"run_training_pipeline\",\n",
    "                               \"ParameterAssignments\": {\"reg_rate\": 0.1}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Programar Pipelines\n",
    "\n",
    "Después de publicar una canalización, puede iniciarla a petición a través de su punto de conexión REST, o puede hacer que la canalización se ejecute automáticamente en función de una programación periódica o en respuesta a actualizaciones de datos.\n",
    "\n",
    "- **Programación de una canalización para intervalos periódicos**\n",
    "\n",
    "    Para programar una canalización para que se ejecute a intervalos periódicos, debe definir un ScheduleRecurrence que determine la frecuencia de ejecución y usarlo para crear un Schedule.\n",
    "\n",
    "Por ejemplo, el código siguiente programa una ejecución diaria de una canalización publicada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import ScheduleRecurrence, Schedule\n",
    "\n",
    "daily = ScheduleRecurrence(frequency='Day', interval=1)\n",
    "pipeline_schedule = Schedule.create(ws, name='Daily Training',\n",
    "                                        description='trains model every day',\n",
    "                                        pipeline_id=published_pipeline.id,\n",
    "                                        experiment_name='Training_Pipeline',\n",
    "                                        recurrence=daily)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Desencadenar una ejecución de canalización en los cambios de datos**\n",
    "\n",
    "    Para programar una canalización para que se ejecute cada vez que cambien los datos, debe crear una programación que supervise una ruta de acceso especificada en un almacén de datos, como se muestra a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "from azureml.pipeline.core import Schedule\n",
    "\n",
    "training_datastore = Datastore(workspace=ws, name='blob_data')\n",
    "pipeline_schedule = Schedule.create(ws, name='Reactive Training',\n",
    "                                    description='trains model on data change',\n",
    "                                    pipeline_id=published_pipeline_id,\n",
    "                                    experiment_name='Training_Pipeline',\n",
    "                                    datastore=training_datastore,\n",
    "                                    path_on_datastore='data/training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
