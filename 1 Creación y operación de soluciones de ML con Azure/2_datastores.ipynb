{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Almacenes de datos\n",
    "\n",
    "Concepto\n",
    "Los almacenes de datos de AzureML son abstracciones para fuentes de datos en la nube.\n",
    "\n",
    "Funciones:\n",
    "- Encapsulan información para la conexión a fuentes de datos.\n",
    "- Permiten acceso directo a través del SDK de AzureML.\n",
    "- Facilitan la carga y descarga de datos.\n",
    "\n",
    "Tipos de Almacenes de Datos Soportados:\n",
    "- Azure Blob Storage\n",
    "- Azure File Storage\n",
    "- Azure Data Lake Store\n",
    "- Azure SQL Database\n",
    "- Azure Databricks File System (DBFS)\n",
    "- Entre los mas usados.. (Para la lista completa, consulte la documentación de AzureML)\n",
    "\n",
    "Almacenes de Datos Predeterminados:\n",
    "- Cada espacio de trabajo tiene dos almacenes predeterminados:\n",
    "  - Contenedor Blob de Azure Storage\n",
    "  - Contenedor de archivos de Azure Storage\n",
    "- Se utilizan para el almacenamiento del sistema de AzureML.\n",
    "\n",
    "Tercer Almacén de Datos:\n",
    "Se agrega automáticamente al usar conjuntos de datos de ejemplo.\n",
    "\n",
    "Uso en Proyectos:\n",
    "- En la mayoría de los proyectos, necesitará usar sus propias fuentes de datos.Las razones pueden ser:\n",
    "  1. Manejar mayores volúmenes de datos.\n",
    "  2. Integrar la solución con datos de aplicaciones existentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear y administrar Almacenes de datos\n",
    "\n",
    "#### Registro:\n",
    "\n",
    "- Interfaz gráfica de AzureML Studio.\n",
    "- SDK de AzureML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registrar un contenedor Blob de Azure Storage llamado \"blob_data\"\n",
    "from azureml.core import Workspace, Datastore\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Registramos el nuevo Datastore\n",
    "blob_ds = Datastore.register_azure_blob_container(\n",
    "    workspace=ws, \n",
    "    datastore_name='blob_data', \n",
    "    container_name='data', \n",
    "    account_name='storageaccountname', \n",
    "    container_name='data_container', \n",
    "    account_name='az_store...', \n",
    "    account_key='storageaccountkey'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Visualización y Gestión:\n",
    "\n",
    "Realizable mediante:\n",
    "- AzureML Studio.\n",
    "- SDK de AzureML.\n",
    "\n",
    "Ejemplo: Listar nombres de almacenes con un bucle for y obtener referencias individuales con el método get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con '/bin/python3' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Listamos los datastores registrados\n",
    "for ds_name in ws.datastores:\n",
    "    print(ds_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Almacenamiento Predeterminado:\n",
    "\n",
    "Incluido en todo espacio de trabajo que se crea.\n",
    "- Recuperable con el método get_default_datastore.\n",
    "- Inicialmente es \"workspace blob store datastore\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos el datastore recién registrado\n",
    "blob_store = Datastore.get(ws, datastore_name='blob_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consideraciones para la Planificación:\n",
    "\n",
    "Azure Blob Storage:\n",
    "- Almacenamiento premium: mejor rendimiento I/O para grandes conjuntos de datos (mayor costo y limitaciones de replicación).\n",
    "\n",
    "Formato de Archivos:\n",
    "- Parquet generalmente ofrece mejor rendimiento que CSV.\n",
    "\n",
    "Acceso y Cambio de Predeterminado:\n",
    "- Acceso por nombre del almacén de datos.\n",
    "- Cambio de predeterminado con el método set_default_datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuramos el datastore por defecto\n",
    "ws.set_default_datastore('blob_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio practico - trabajar con datasets\n",
    "\n",
    "##### Creacion, carga y versiones\n",
    "1. Creación de conjuntos de datos de archivos:\n",
    "   \n",
    "    Los datasets son objetos de datos empaquetados con versiones que se pueden usar en experimentos y pipelines. Son la forma recomendada para trabajar con datos en Azure Machine Learning.\n",
    "\n",
    "    Tipos:\n",
    "\n",
    "    - **Tabular**: Para datos con estructura consistente (ej. DataFrames de Pandas).\n",
    "    - **Archivo**: Para datos no estructurados o procesamiento a nivel de archivo (ej. imágenes).\n",
    "\n",
    "    El código en Python muestra cómo crear un dataset tabular a partir de dos rutas de archivo y registrarlo en el espacio de trabajo.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con '/bin/python3' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Para crear un conjunto de datos tabular mediante el SDK, se usa el  método from_delimited_files de la  clase Dataset.Tabular, como se muestra a continuación:\n",
    "\n",
    "from azureml.core import Dataset\n",
    "\n",
    "blob_ds = ws.get_default_datastore()\n",
    "\n",
    "csv_paths = [(blob_ds, 'data/files/current_data.csv'),          # Archivo current_data.csv en la  carpeta data/files/   \n",
    "            (blob_ds, 'data/files/archive/*.csv')]              # Todos los archivos .csv en la  carpeta data/files/archive/\n",
    "\n",
    "# Usando el método from_delimited_files\n",
    "tab_ds = Dataset.Tabular.from_delimited_files(path=csv_paths)   # from_delimited_files de la  clase Dataset.Tabular\n",
    "\n",
    "# \n",
    "tab_ds = tab_ds.register(workspace=ws, name='csv_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con '/bin/python3' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Para crear un conjunto de datos de archivo mediante el SDK, use el  método from_files de la  clase Dataset.File\n",
    "\n",
    "from azureml.core import Dataset\n",
    "\n",
    "blob_ds = ws.get_default_datastore()\n",
    "\n",
    "file_ds = Dataset.File.from_files(path=(blob_ds, 'data/files/images/*.jpg'))    # from_files de la  clase Dataset.File\n",
    "\n",
    "file_ds = file_ds.register(workspace=ws, name='img_files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Recuperación de conjuntos de datos registrados:\n",
    "\n",
    "    Se puede recuperar el dataset mediante cualquiera de las siguientes técnicas:\n",
    "\n",
    "    - El  atributo `datasets` dictionary de un objeto Workspace.\n",
    "    - Método `get_by_name` o `get_by_id` de la clase Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset\n",
    "\n",
    "ws = Workspace.from_config()                        # Obtenemos el objeto Workspace\n",
    "\n",
    "ds1 = ws.datasets['csv_table']                      # Seleccionamos el conjunto de datos por su nombre \n",
    "\n",
    "ds2 = Dataset.get_by_name(ws, 'img_files')          # Seleccionamos el conjunto de datos por el nombre la su clase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Versionado de conjuntos de datos:\n",
    "\n",
    "    Los conjuntos de datos se pueden versionar, lo que le permite realizar un seguimiento de las versiones históricas y reproducir esos experimentos con datos en el mismo estado.\n",
    "\n",
    "    Puede crear una nueva versión de un conjunto de datos registrándolo con el mismo nombre que un conjunto de datos registrado anteriormente y especificando la  propiedad `create_new_version`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En este ejemplo, los archivos .png de la carpeta images se han agregado al conjunto de datos img_paths utilizado.\n",
    "\n",
    "img_paths = [(blob_ds, 'data/files/images/*.jpg'),\n",
    "             (blob_ds, 'data/files/images/*.png')]\n",
    "\n",
    "file_ds = Dataset.File.from_files(path=img_paths)                                   # Usamos el método from_files para registrar los archivos jpg y png\n",
    "\n",
    "file_ds = file_ds.register(workspace=ws, name='img_files', create_new_version=True) # creamos una nueva versión del conjunto de datos img_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Recuperación de versiones específicas de conjuntos de datos:\n",
    "\n",
    "    Podemos recuperar una versión específica de un conjunto especificando el parámetro version en el método `get_by_name` de la clase Dataset.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ds = Dataset.get_by_name(workspace=ws, name='img_files', version=2)            # Obtenemos la versión 2 del conjunto de datos img_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Manejo y uso\n",
    "\n",
    "5. Trabajar con conjuntos de datos \n",
    "\n",
    "    Podemos leer datos directamente de un conjunto de datos tabular convirtiéndolos en un marco de datos de Pandas o Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = tab_ds.to_pandas_dataframe()\n",
    "print(df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Pasar un conjunto de datos tabular a un script\n",
    "   \n",
    "Cuando necesitemos acceder a un conjunto de datos mediante un script, debemos pasarle el conjunto de datos al script. \n",
    "\n",
    "Hay dos maneras de hacerlo:\n",
    "\n",
    "- **Usar un argumento de script para un conjunto de datos tabulares**\n",
    "\n",
    "    Podemos pasar el conjunto de datos como argumento de script. \n",
    "    \n",
    "    Cuando se adopta este enfoque, el argumento recibido por el script es el identificador único del conjunto de datos en el área de trabajo. En el script, puede obtener el área de trabajo del contexto de ejecución y usarlo para recuperar el conjunto de datos por su identificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ScriptRunConfig\n",
    "\n",
    "env = Environment('my_env')                                                         # Creamos un objeto Environment  \n",
    "packages = CondaDependencies.create(conda_packages=['pip'],                         # Creamos un objeto CondaDependencies con las dependencias necesarias\n",
    "                                    pip_packages=['azureml-defaults',\n",
    "                                                  'azureml-dataprep[pandas]'])\n",
    "env.python.conda_dependencies = packages                                            # Asignamos las dependencias al entorno                             \n",
    "\n",
    "script_config = ScriptRunConfig(source_directory='my_dir',                          # Creamos un objeto ScriptRunConfig   \n",
    "                                script='script.py',\n",
    "                                arguments=['--ds', tab_ds],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script\n",
    "\n",
    "from azureml.core import Run, Dataset\n",
    "\n",
    "parser.add_argument('--ds', type=str, dest='dataset_id')                            # Agregamos un argumento para el conjunto de datos\n",
    "args = parser.parse_args()\n",
    "\n",
    "run = Run.get_context()\n",
    "ws = run.experiment.workspace\n",
    "dataset = Dataset.get_by_id(ws, id=args.dataset_id)                                 # Obtenemos el conjunto de datos por su id\n",
    "data = dataset.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Usar una entrada con nombre para un conjunto de datos tabular**\n",
    "\n",
    "    Como alternativa, podemos pasar un conjunto de datos tabular como una entrada con nombre. En este enfoque, se utiliza el  método `as_named_input` del conjunto de datos para especificar un nombre. \n",
    "\n",
    "    A continuación, en el script, puede recuperar el conjunto de datos por nombre de la colección `input_datasets` del contexto de ejecución sin necesidad de recuperarlo del área de trabajo. Tenga en cuenta que si usa este enfoque, aún debe incluir un argumento de script para el conjunto de datos, aunque en realidad no lo use para recuperar el conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ScriptRunConfig\n",
    "\n",
    "env = Environment('my_env')\n",
    "packages = CondaDependencies.create(conda_packages=['pip'],\n",
    "                                    pip_packages=['azureml-defaults',\n",
    "                                                  'azureml-dataprep[pandas]'])\n",
    "env.python.conda_dependencies = packages\n",
    "\n",
    "script_config = ScriptRunConfig(source_directory='my_dir',\n",
    "                                script='script.py',\n",
    "                                arguments=['--ds', tab_ds.as_named_input('my_dataset')],    # Pasamos el conjunto de datos como argumento\n",
    "                                environment=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script\n",
    "\n",
    "from azureml.core import Run\n",
    "\n",
    "parser.add_argument('--ds', type=str, dest='ds_id')\n",
    "args = parser.parse_args()\n",
    "\n",
    "run = Run.get_context()\n",
    "dataset = run.input_datasets['my_dataset']                                          # Obtenemos el conjunto de datos por su nombre\n",
    "data = dataset.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Trabajar con conjuntos de datos de archivos\n",
    "\n",
    "    Para trabajar con un conjunto de datos de archivos, podemos utilizar el método `to_path()` para devolver una lista de las rutas de archivo encapsuladas por el conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in file_ds.to_path():\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Pasar un conjunto de datos de archivo a un script \n",
    "\n",
    "    Al igual que con los conjuntos de datos tabulares, hay dos formas de pasar un conjunto de datos de archivo a un script. Sin embargo, hay algunas diferencias clave en la forma en que se pasa el conjunto de datos.\n",
    "\n",
    "    - **Usar un argumento de script para un conjunto de datos de archivos**\n",
    "\n",
    "        Puede pasar un conjunto de datos de archivos como argumento de script. A diferencia de lo que ocurre con un conjunto de datos tabular, debe especificar un modo para el argumento del conjunto de datos de archivo, que puede ser `as_download` o `as_mount`. Esto proporciona un punto de acceso que el script puede utilizar para leer los archivos del conjunto de datos. \n",
    "\n",
    "        En la mayoría de los casos, se ha de `as_download`, que copia los archivos en una ubicación temporal en el proceso donde se ejecuta el script. Sin embargo, si está trabajando con una gran cantidad de datos para los que es posible que no haya suficiente espacio de almacenamiento en el proceso, usaremos `as_mount` para transmitir los archivos directamente desde su origen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ScriptRunConfig\n",
    "\n",
    "env = Environment('my_env')\n",
    "packages = CondaDependencies.create(conda_packages=['pip'],\n",
    "                                    pip_packages=['azureml-defaults',\n",
    "                                                  'azureml-dataprep[pandas]'])\n",
    "env.python.conda_dependencies = packages\n",
    "\n",
    "script_config = ScriptRunConfig(source_directory='my_dir',\n",
    "                                script='script.py',\n",
    "                                arguments=['--ds', file_ds.as_download()],                          # Pasamos el conjunto de datos como argumento usando el método as_download\n",
    "                                environment=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Script\n",
    "\n",
    "from azureml.core import Run\n",
    "import glob\n",
    "\n",
    "parser.add_argument('--ds', type=str, dest='ds_ref')\n",
    "args = parser.parse_args()\n",
    "run = Run.get_context()\n",
    "\n",
    "imgs = glob.glob(args.ds_ref + \"/*.jpg\")                                                            # Obtenemos los archivos del conjunto de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Usar una entrada con nombre para un conjunto de datos de archivos**\n",
    "\n",
    "    También puede pasar un conjunto de datos de archivos como una entrada con nombre. En este enfoque, se utiliza el  método as_named_input del conjunto de datos para especificar un nombre antes de especificar el modo de acceso. A continuación, en el script, puede recuperar el conjunto de datos por nombre de la colección input_datasets del contexto de ejecución  y leer los archivos desde allí. \n",
    "\n",
    "    Al igual que con los conjuntos de datos tabulares, si usa una entrada con nombre, debe incluir un argumento de script para el conjunto de datos, aunque en realidad no lo use para recuperar el conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ScriptRunConfig\n",
    "\n",
    "env = Environment('my_env')\n",
    "packages = CondaDependencies.create(conda_packages=['pip'],\n",
    "                                    pip_packages=['azureml-defaults',\n",
    "                                                  'azureml-dataprep[pandas]'])\n",
    "env.python.conda_dependencies = packages\n",
    "\n",
    "script_config = ScriptRunConfig(source_directory='my_dir',\n",
    "                                script='script.py',\n",
    "                                arguments=['--ds', file_ds.as_download()],                       # Pasamos el conjunto de datos como argumento usando el método as_download\n",
    "                                environment=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script\n",
    "\n",
    "from azureml.core import Run\n",
    "import glob\n",
    "\n",
    "parser.add_argument('--ds', type=str, dest='ds_ref')\n",
    "args = parser.parse_args()\n",
    "run = Run.get_context()\n",
    "\n",
    "imgs = glob.glob(args.ds_ref + \"/*.jpg\")                                                           # Obtenemos los archivos del conjunto de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Usar una entrada con nombre para un conjunto de datos de archivos\n",
    "    \n",
    "    En este enfoque, se utiliza el  método `as_named_input` del conjunto de datos para especificar un nombre antes de especificar el modo de acceso. A continuación, en el script, puede recuperar el conjunto de datos por nombre de la colección `input_datasets` del contexto de ejecución y leer los archivos desde allí. \n",
    "\n",
    "    Al igual que con los conjuntos de datos tabulares, si usa una entrada con nombre, debe incluir un argumento de script para el conjunto de datos, aunque en realidad no lo use para recuperar el conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ScriptRunConfig\n",
    "\n",
    "env = Environment('my_env')\n",
    "packages = CondaDependencies.create(conda_packages=['pip'],\n",
    "                                    pip_packages=['azureml-defaults',\n",
    "                                                  'azureml-dataprep[pandas]'])\n",
    "env.python.conda_dependencies = packages\n",
    "\n",
    "script_config = ScriptRunConfig(source_directory='my_dir',\n",
    "                                script='script.py',\n",
    "                                arguments=['--ds', file_ds.as_named_input('my_ds').as_download()],    # Pasamos el conjunto de datos como argumento usando el método as_named_input\n",
    "                                environment=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con '/bin/python3' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Script\n",
    "\n",
    "from azureml.core import Run\n",
    "import glob\n",
    "\n",
    "parser.add_argument('--ds', type=str, dest='ds_ref')\n",
    "args = parser.parse_args()\n",
    "run = Run.get_context()\n",
    "\n",
    "dataset = run.input_datasets['my_ds']                                                        # Obtenemos el conjunto de datos por su nombre \n",
    "imgs= glob.glob(dataset + \"/*.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
