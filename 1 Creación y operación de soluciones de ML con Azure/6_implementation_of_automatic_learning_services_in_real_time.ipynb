{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementacion de servicios de aprendizaje automatico en tiempo real con Azure ML\n",
    "\n",
    "- **Inferencia**: Uso de un modelo entrenado para predecir etiquetas para datos nuevos (no usados durante el entrenamiento).\n",
    "\n",
    "- **Servicio de Inferencia en Tiempo Real**: Permite a las aplicaciones solicitar predicciones inmediatas al modelo para datos individuales o pequeños conjuntos.\n",
    "\n",
    "Despliegue del Modelo:\n",
    "\n",
    "- **Contenedorizado en AKS (Azure Kubernetes Services)**: Plataforma de orquestación de contenedores para implementar y administrar aplicaciones en contenedores.\n",
    "\n",
    "- **Servicio de Inferencia**: Punto de acceso para que las aplicaciones consuman el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un modelo puede implementarse como un servicio web en tiempo real en varios destinos, incluyendo localmente, Azure ML, ACI, AKS, una función de Azure o un módulo IoT. Azure ML contenedores para la implementación, empaquetando el modelo y el código en una imagen que se puede implementar en un contenedor en el destino seleccionado.\n",
    "\n",
    "Nota\n",
    "\n",
    "    La implementación en un servicio local, una instancia informática o una ACI es una buena opción para las pruebas y el desarrollo. Para producción, se debe implementar en un destino que satisfaga las necesidades específicas de rendimiento, escalabilidad y seguridad de la arquitectura de la aplicación.\n",
    "\n",
    "Para implementar un modelo como un servicio de inferencia en tiempo real, debe realizar las siguientes tareas:\n",
    "\n",
    "#### 1. Registro de un modelo entrenado\n",
    "Después de entrenar correctamente un modelo, debe registrarlo en el área de trabajo de Azure ML. Su servicio en tiempo real podrá cargar el modelo cuando sea necesario.\n",
    "\n",
    "Para registrar un modelo a partir de un archivo local, puede utilizar el método `register` del  objeto Model como se muestra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuando tienes un modelo que se ha entrenado fuera de Azure ML y quieres registrar el modelo en tu espacio de trabajo.\n",
    "\n",
    "from azureml.core import Model\n",
    "\n",
    "classification_model = Model.register(workspace=ws,                         # Registramos el modelo en el espacio de trabajo\n",
    "                       model_name='classification_model',\n",
    "                       model_path='model.pkl',                              # ruta local\n",
    "                       description='A classification model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternativamente, si tenemos referencia al `run` utilizado para entrenar el modelo, podemos utilizar su método `register_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run en Azure ML se refiere a una ejecución individual de un script de entrenamiento o un pipeline.\n",
    "# Cuando se llama a run.register_model, el modelo se registra junto con metadatos sobre el Run específico, como los parámetros de entrenamiento utilizados. \n",
    "# Esto puede ser útil para rastrear cómo se creó el modelo.\n",
    "\n",
    "run.register_model( model_name='classification_model',                      # Registramos el modelo en el espacio de trabajo\n",
    "                    model_path='outputs/model.pkl',                         # run ruta de salida\n",
    "                    description='A classification model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Definir una configuración de inferencia\n",
    "\n",
    "Los modelos que se implementan como un servicio, constan de:\n",
    "\n",
    "1. Un script para cargar el modelo y devolver predicciones para los datos enviados.\n",
    "2. Un entorno en el que se ejecutará el script.\n",
    "\n",
    "##### Crear un script de entrada\n",
    "\n",
    "Creando la secuencia de comandos de entrada (tambien llamada **secuencia de comandos de puntuación**), para el servicio como un archivo de Python (.py), necesitamos de:\n",
    "\n",
    "   - `init()`: Se llama cuando se inicializa el servicio.\n",
    "   - `run(raw_data)`: Se llama cuando se envían nuevos datos al servicio.\n",
    "\n",
    "Normalmente, se usa la función `init` para cargar el modelo desde el registro de modelos y se usa la función `run` para generar predicciones a partir de los datos de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "from azureml.core.model import Model\n",
    "\n",
    "# llamamos a la función init() para cargar el modelo en la memoria cuando se inicia el contenedor\n",
    "def init():\n",
    "    global model\n",
    "    model_path = Model.get_model_path('classification_model')                   # Obtenemos la ruta del modelo y lo cargamos en memoria\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "\n",
    "# llamamos a la función run() para obtener una predicción para los datos de entrada cuando se realiza una solicitud al servicio web\n",
    "def run(raw_data):\n",
    "    data = np.array(json.loads(raw_data)['data'])                               # Convertimos los datos de entrada en un array de numpy\n",
    "    predictions = model.predict(data)                                           # Realizamos las predicciones\n",
    "    return predictions.tolist()                                                 # Devolvemos las predicciones como una lista serializable en JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Crear un entorno\n",
    "\n",
    "Requerimos de un entorno de Python en el que ejecutar el script de entrada, que se puede configurar mediante el archivo de configuración de Conda. \n",
    "\n",
    "Una manera sencilla de crear este archivo es usar una clase `CondaDependencies` para crear un entorno predeterminado (que incluye el paquete `azureml-defaults` y paquetes de uso común como `numpy` y `pandas`, pero podemos agregar cualquier otro paquete necesario), a continuación, serealizamos el entorno en una cadena y lo guardamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# Creamos un objeto CondaDependencies y agregamos las dependencias necesarias\n",
    "myenv = CondaDependencies()\n",
    "myenv.add_conda_package(\"scikit-learn\")\n",
    "\n",
    "# Guardamos el archivo de especificación de entorno\n",
    "env_file = 'service_files/env.yml'\n",
    "with open(env_file,\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())\n",
    "print(\"Saved dependency info in\", env_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combinar el script y el entorno en un InferenceConfig\n",
    "\n",
    "Después de crear el script de entrada y el archivo de configuración del entorno, podemos combinarlos en un `InferenceConfig` para el servicio de la siguiente manera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "# Creamos un objeto InferenceConfig con la configuración necesaria\n",
    "classifier_inference_config = InferenceConfig(runtime= \"python\",\n",
    "                                              source_directory = 'service_files',\n",
    "                                              entry_script=\"score.py\",\n",
    "                                              conda_file=\"env.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Definir una configuración de implementación\n",
    "\n",
    "Con el script y el archivo de entorno, ahora debemos configurar el proceso en el que se implementará el servicio. \n",
    "\n",
    "Si vamos a realizar la implementación en un clúster de AKS, tenemos que crear el clúster y un destino de proceso para él antes de realizar la implementación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AksCompute\n",
    "\n",
    "# Creamos un objeto AksCompute con la configuración necesaria para el clúster de AKS\n",
    "cluster_name = 'aks-cluster'\n",
    "compute_config = AksCompute.provisioning_configuration(location='eastus')\n",
    "production_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "production_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez creado el destino del proceso, podemos definir la configuración de implementación. Esta configuración establece la especificación del proceso, específica para el destino, para la implementación en contenedores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AksWebservice\n",
    "\n",
    "# Creamos un objeto AksWebservice con la configuración necesaria para el servicio web\n",
    "classifier_deploy_config = AksWebservice.deploy_configuration(cpu_cores = 1,\n",
    "                                                              memory_gb = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para configurar una implementación de ACI, el código es similar. \n",
    "\n",
    "No necesitas crear un destino de proceso de ACI explícitamente y debes usar la clase `deploy_configuration` del espacio de nombres `azureml.core.webservice.AciWebservice`. De manera similar, puedes usar el espacio de nombres `azureml.core.webservice.LocalWebservice` para configurar un servicio local basado en Docker.\n",
    "\n",
    "Si queremos implementar un modelo en una función de Azure, no necesitas crear una configuración de implementación. En su lugar, debes empaquetar el modelo según el tipo de desencadenador de función que quieras usar. Esta funcionalidad está en versión preliminar en el momento de escribir este artículo. Para obtener más información, consulta la sección [“Implementación de un modelo de aprendizaje automático en Azure Functions”](https://learn.microsoft.com/es-es/azure/machine-learning/how-to-deploy-online-endpoints?view=azureml-api-2&tabs=azure-cli) en la documentación de Azure ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implementar el modelo\n",
    "Una vez preparada toda la configuración, podemos implementar el modelo. La forma más fácil de hacerlo es llamar al método deploy de la clase `Model`, de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'azureml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mazureml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m ws\u001b[38;5;241m.\u001b[39mmodels[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification_model\u001b[39m\u001b[38;5;124m'\u001b[39m]                                       \u001b[38;5;66;03m# Obtenemos el modelo por su nombre\u001b[39;00m\n\u001b[1;32m      4\u001b[0m service \u001b[38;5;241m=\u001b[39m Model\u001b[38;5;241m.\u001b[39mdeploy(workspace\u001b[38;5;241m=\u001b[39mws,                                            \u001b[38;5;66;03m# Desplegamos el servicio web en el clúster de AKS\u001b[39;00m\n\u001b[1;32m      5\u001b[0m                        name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassifier-service\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m                        models \u001b[38;5;241m=\u001b[39m [model],\n\u001b[1;32m      7\u001b[0m                        inference_config \u001b[38;5;241m=\u001b[39m classifier_inference_config,\n\u001b[1;32m      8\u001b[0m                        deployment_config \u001b[38;5;241m=\u001b[39m classifier_deploy_config,\n\u001b[1;32m      9\u001b[0m                        deployment_target \u001b[38;5;241m=\u001b[39m production_cluster)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'azureml'"
     ]
    }
   ],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "model = ws.models['classification_model']                                       # Obtenemos el modelo por su nombre\n",
    "service = Model.deploy(workspace=ws,                                            # Desplegamos el servicio web en el clúster de AKS\n",
    "                       name = 'classifier-service',\n",
    "                       models = [model],\n",
    "                       inference_config = classifier_inference_config,\n",
    "                       deployment_config = classifier_deploy_config,\n",
    "                       deployment_target = production_cluster)\n",
    "service.wait_for_deployment(show_output = True)                                 # Esperamos a que se complete el despliegue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de ACI o servicios locales, puede omitir el parámetro `deployment_target` (o establecerlo en **None**).\n",
    "\n",
    "[Más into sobre la implementación de modelos con Azure Machine Learning](https://learn.microsoft.com/es-es/azure/machine-learning/how-to-deploy-online-endpoints?view=azureml-api-2&tabs=azure-cli)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Consumo de un servicio de inferencia en tiempo real\n",
    "\n",
    "Después de implementar el servicio en tiempo real, puede consumirlo desde aplicaciones cliente para predecir etiquetas para nuevos casos de datos.\n",
    "\n",
    "##### Uso del SDK de Azure Machine Learning\n",
    "\n",
    "Para realizar pruebas, puede usar el SDK de Azure ML para llamar a un servicio web a través del método run de un objeto `WebService` que haga referencia al servicio implementado. Normalmente, los datos se envían al método `run` en formato JSON con la siguiente estructura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"data\":[\n",
    "      [0.1,2.3,4.1,2.0],  // 1st case\n",
    "      [0.2,1.8,3.9,2.1],  // 2nd case\n",
    "      ...\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La respuesta del método `run` sera una colección JSON con una predicción para cada caso que se envió en los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En este ejemplo se llama a un servicio y se muestra la respuesta\n",
    "\n",
    "import json\n",
    "\n",
    "# matriz de datos nuevos\n",
    "x_new = [[0.1,2.3,4.1,2.0],\n",
    "         [0.2,1.8,3.9,2.1]]\n",
    "\n",
    "\n",
    "json_data = json.dumps({\"data\": x_new})                     # Convertimos la matriz en un formato JSON serializable\n",
    "\n",
    "response = service.run(input_data = json_data)              # Llamamos al servicio web con los datos de entrada\n",
    "\n",
    "predictions = json.loads(response)                          # Convertimos la respuesta en un objeto JSON\n",
    "\n",
    "for i in range(len(x_new)):                                 # Mostramos las predicciones para cada conjunto de datos\n",
    "    print (x_new[i], predictions[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Uso de un punto de conexión REST \n",
    "\n",
    "En producción, la mayoría de las aplicaciones cliente no incluirán el SDK de Azure ML y consumirán el servicio a través de su interfaz REST. \n",
    "\n",
    "Podemos determinar el punto de conexión de un servicio implementado en Azure ML Studio o recuperar la propiedad `scoring_uri` del objeto Webservice en el SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En este ejemplo se muestra cómo obtener la dirección URL del servicio web\n",
    "\n",
    "endpoint = service.scoring_uri\n",
    "print(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el punto de conexión conocido, puede usar una solicitud HTTP POST con datos JSON para llamar al servicio. En el siguiente ejemplo se muestra cómo hacerlo con Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "x_new = [[0.1,2.3,4.1,2.0],\n",
    "         [0.2,1.8,3.9,2.1]]\n",
    "\n",
    "json_data = json.dumps({\"data\": x_new})                         # Convertimos la matriz en un formato JSON serializable\n",
    "\n",
    "request_headers = { 'Content-Type':'application/json' }         # Set the content type in the request headers\n",
    "\n",
    "response = requests.post(url = endpoint,                        # Realizamos la solicitud POST al servicio web\n",
    "                         data = json_data,\n",
    "                         headers = request_headers)\n",
    "\n",
    "predictions = json.loads(response.json())                       # Convertimos la respuesta en un objeto JSON\n",
    "\n",
    "for i in range(len(x_new)):                                     # Imprimimos las predicciones para cada conjunto de datos\n",
    "    print ((x_new[i]), predictions[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Autenticación\n",
    "\n",
    "En producción, es probable que queramos restringir el acceso a los servicios mediante la aplicación de autenticación. Hay dos tipos de autenticación que se pueden aplicar:\n",
    "- **Clave**: las solicitudes se autentican especificando la clave asociada al servicio.\n",
    "- **Token**: se autentican proporcionando un token web JSON (JWT).\n",
    "\n",
    "De forma predeterminada, la autenticación está deshabilitada para los servicios ACI y se establece en la autenticación basada en claves para los servicios de AKS (para los que las claves principal y secundaria se generan automáticamente). Opcionalmente, puede configurar un servicio de AKS para usar la autenticación basada en tokens (que no es compatible con los servicios ACI).\n",
    "\n",
    "Suponiendo que tenemos una sesión autenticada establecida con el área de trabajo, podemos recuperar las claves del servicio mediante el método `get_keys` del objeto WebService asociado al servicio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_key, secondary_key = service.get_keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de la autenticación basada en tokens, la aplicación cliente debe usar la autenticación de entidad de servicio para comprobar su identidad a través de Azure Active Directory (Azure AD) y llamar al  método `get_token` del servicio para recuperar un token de tiempo limitado.\n",
    "\n",
    "Para realizar una llamada autenticada al punto de conexión REST del servicio, debemos incluir la clave o el token en el encabezado de la solicitud de la siguiente manera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "x_new = [[0.1,2.3,4.1,2.0],\n",
    "         [0.2,1.8,3.9,2.1]]\n",
    "\n",
    "json_data = json.dumps({\"data\": x_new})\n",
    "\n",
    "request_headers = { \"Content-Type\":\"application/json\",                      # incluimos la clave de autenticación en la solicitud\n",
    "                    \"Authorization\":\"Bearer \" + key_or_token }\n",
    "\n",
    "response = requests.post(url = endpoint,                                    # llama al servicio web con los datos de entrada\n",
    "                         data = json_data,\n",
    "                         headers = request_headers)\n",
    "\n",
    "predictions = json.loads(response.json())\n",
    "\n",
    "for i in range(len(x_new)):\n",
    "    print ((x_new[i]), predictions[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Implementación de un modelo como servicio web en tiempo real\n",
    "\n",
    "Destinos de proceso\n",
    "- Proceso local\n",
    "- Instancia de proceso de Azure Machine Learning\n",
    "- Instancia de Azure Container Instance (ACI)\n",
    "- Clúster de Azure Kubernetes Service (AKS)\n",
    "- Función de Azure\n",
    "- Módulo de Internet de las cosas (IoT)\n",
    "\n",
    "Mecanismo de implementación\n",
    "- Contenedores\n",
    "- Empaquetado del modelo y el código como una imagen\n",
    "- Implementación en un contenedor en el destino elegido\n",
    "\n",
    "Consideraciones:\n",
    "- Pruebas y desarrollo: servicio local, instancia informática o ACI\n",
    "- Producción: destino que cumpla con los requisitos de rendimiento, escalabilidad y seguridad de la aplicación\n",
    "\n",
    "Tareas para implementar un servicio de inferencia en tiempo real:\n",
    "1. **Registrar el modelo y el entorno**: Almacenar el modelo y su entorno de ejecución en Azure Machine Learning.\n",
    "2. **Crear una configuración de inferencia**: Especificar cómo se ejecuta el modelo en tiempo real.\n",
    "3. **Crear una configuración de destino**: Seleccionar el destino de proceso y las opciones de implementación.\n",
    "4. **Implementar el servicio**: Implementar el modelo como un servicio web en el destino elegido.\n",
    "5. **Probar el servicio**: Enviar solicitudes de prueba al servicio y verificar las predicciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
